# TgBot Project Context
Generated on: 2025-03-09 11:01:26

## Project Overview
- **Name**: TgBot
- **Description**: Telegram bot for monitoring cryptocurrency messages and tracking token mentions
**Main Components:**
- Telegram Monitor
- Database Storage
- Price Tracker
- Celery Workers

## Database Schemas

### File: sql/crypto_db_schema.sql
Tables defined:
- `public.telegram_groups`
- `public.telegram_messages`
- `public.token_calls`
- `public.tokens`

## .docker Files

### .docker/config.json
```json
{
  "credsStore": "desktop.exe"
}

```

### .docker/features.json
```json
[Error reading file: [Errno 2] No such file or directory: '/home/marshall/TgBot/.docker/features.json']
```

## config Files

### config/logging.py
```python
"""
Logging configuration
"""
import logging
import sys
from pathlib import Path

# Create logs directory if it doesn't exist
logs_dir = Path(__file__).resolve().parent.parent / "logs"
logs_dir.mkdir(exist_ok=True)

def configure_logging():
    """Configure and return a logger"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(logs_dir / 'telegram_bot.log')
        ]
    )
    return logging.getLogger(__name__)

```

### config/settings.py
```python
"""
Configuration file for application settings
"""
import os
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables
load_dotenv()

# Application root directory
BASE_DIR = Path(__file__).resolve().parent.parent

# Telegram API settings
TELEGRAM_API_ID = int(os.getenv("API_ID", "28644650"))
TELEGRAM_API_HASH = os.getenv("API_HASH", "e963f9b807bcf9d665b1d20de66f7c69")
TELEGRAM_PHONE = os.getenv("PHONE_NUMBER")
SESSION_PATH = os.getenv("SESSION_PATH", "./session/session_name")

# Database settings
DB_CONFIG = {
    "host": os.getenv("PG_HOST", "timescaledb"),
    "port": os.getenv("PG_PORT", "5432"),
    "user": os.getenv("PG_USER", "bot"),
    "password": os.getenv("PG_PASSWORD", "bot1234"),
    "database": os.getenv("PG_DATABASE", "crypto_db")
}

# API Settings
DEXSCREENER_API_TIMEOUT = 15  # seconds


# Add these settings to the existing file

# MongoDB settings
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://mongo:27017/')
MONGO_DB = os.getenv('MONGO_DB', 'tgbot_db')

# Celery settings
CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://redis:6379/0')
CELERY_RESULT_BACKEND = os.getenv('CELERY_RESULT_BACKEND', 'redis://redis:6379/0')

```

### config/groups.py
```python
"""
Configuration file containing Telegram group IDs and names
"""

# Dictionary mapping Telegram group IDs to their readable names
TELEGRAM_GROUPS = {
    -1002124780831: "printoooorrrr gambles",
    -1002161891429: "printoooorrrr calls",
    -1002470555079: "HAVEN (PRV TRAILBLAZER )",
    -1002413987457: "ai ‰ø°Âè∑",
    -1001669758312: "CTü¶Ö zin alpha entries",
    -1001355642881: "CryptoDeus Gems üíéü¶ç",
    -1001756488143: "- SOL -",
    -1001593046999: "VultureCalls",
    -1002390818052: "Obito Calls",
    -1001637465785: "Vulture's Risky Hunter",
    -1001759083119: "$nipers",
    -1001807628627: "Degen Apes",
    -1001870127953: "wat is des",
    -1001710178652: "Obito calls",
    -1001667198684: "Reaper Gems",
    -1001216010763: "Avastar",
    -1001258234189: "1000ùïè Multi Chain",
    -1001539956400: "ApeZone",
    -1002360457432: "MarshallCalls",
    -1002175554746: "SOLANA INSIDER CALLS",
    -1001989363348: "Maythous Degens",
    -1002449259636: "Brokekid Crypto",
    -1002234182572: "Ketchum's Gambles",
    -1001198046393: "Pow's Gem Calls",
    -1001601652682: "Obitos Calls"
}

```

### config/regex_patterns.py
```python
"""
Regular expressions used in the application
"""
import re

# Regex patterns for token and call detection
CALL_PATTERN = re.compile(r"üé≤\s*New\s*Gamble\s*Call", re.IGNORECASE)
RE_CA_BSC_ETH = re.compile(r"\b0x[a-fA-F0-9]{40}\b")
RE_CA_SOL = re.compile(r"\b[1-9A-HJ-NP-Za-km-z]{32,44}\b")
DEX_LINK_REGEX = re.compile(r"https?://dexscreener\.com/(solana|bsc|ethereum)/([^/\s\?]+)")
TINYASTRO_REGEX = re.compile(r"https?://photon-sol\.tinyastro\.io/\w+/lp/([1-9A-HJ-NP-Za-km-z]{32,44})")

```

### config/__init__.py
```python

```

## root Files

### docker-compose.yml
```yaml
services:
  postgres:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_USER=bot
      - POSTGRES_PASSWORD=bot1234
      - POSTGRES_DB=crypto_db
    ports:
      - "5432:5432"
    volumes:
      - ./volumes/postgres-data:/var/lib/postgresql/data
    networks:
      - app-network

  pgadmin:
    image: dpage/pgadmin4
    environment:
      - PGADMIN_DEFAULT_EMAIL=marcelmartino2053@gmail.com
      - PGADMIN_DEFAULT_PASSWORD=admin123
     # Add this to prevent lockouts
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
      - PGADMIN_CONFIG_LOGIN_BANNER="Welcome to PgAdmin"
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - app-network

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - app-network

  mongo:
    image: mongo:latest
    environment:
      - MONGO_INITDB_ROOT_USERNAME=bot  # Match .env MONGO_USER
      - MONGO_INITDB_ROOT_PASSWORD=bot1234  # Match .env MONGO_PASSWORD
    ports:
      - "27017:27017"
    volumes:
      - ./volumes/mongo-data:/data/db
    networks:
      - app-network

  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile.celery
    volumes:
      - ./:/app
    depends_on:
      - postgres
      - redis
      - mongo
    environment:
      - PG_HOST=postgres
      - PG_PORT=5432
      - PG_USER=bot
      - PG_PASSWORD=bot1234
      - PG_DATABASE=crypto_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MONGO_HOST=mongo
      - MONGO_PORT=27017
      - MONGO_USER=bot  # Match .env
      - MONGO_PASSWORD=bot1234
      - MONGO_AUTH_SOURCE=admin
      - MONGO_DB=tgbot_db
      - PYTHONPATH=/app
      - MONGO_COLLECTION_NAME=dexscreener_data
    command: celery -A scripts.price_tracker.celery_app worker --loglevel=info
    networks:
      - app-network

  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile.celery
    volumes:
      - ./:/app
    depends_on:
      - postgres
      - redis
      - mongo
    environment:
      - PG_HOST=postgres
      - PG_PORT=5432
      - PG_USER=bot
      - PG_PASSWORD=bot1234
      - PG_DATABASE=crypto_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MONGO_HOST=mongo
      - MONGO_PORT=27017
      - MONGO_USER=bot
      - MONGO_PASSWORD=bot1234
      - MONGO_COLLECTION_NAME=dexscreener_data
      - PYTHONPATH=/app
    command: celery -A scripts.price_tracker.celery_app beat --loglevel=info
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

```

### start_bot.sh
```bash
#!/bin/bash

# Store PIDs for cleanup
BOT_PID=""

# Function to kill all processes on exit
cleanup() {
    echo "Stopping all processes..."
    
    # Kill the Telegram bot if running
    if [ -n "$BOT_PID" ]; then
        echo "Stopping Telegram bot (PID: $BOT_PID)..."
        kill -TERM $BOT_PID 2>/dev/null || true
    fi
    
    # Stop Celery workers via docker-compose
    echo "Stopping Celery workers..."
    docker-compose stop celery_worker celery_beat
    
    echo "All processes stopped."
    exit 0
}

# Set up signal handling - when Ctrl+C is pressed, run cleanup
trap cleanup SIGINT SIGTERM

# Ensure services are running
echo "Verificando que los servicios est√©n funcionando..."
if ! docker ps | grep -q postgres || ! docker ps | grep -q mongo || ! docker ps | grep -q redis; then
  echo "Iniciando servicios necesarios..."
  docker-compose up -d postgres pgadmin mongo redis
  
  # Give MongoDB time to initialize
  echo "Esperando 5 segundos para que MongoDB se inicialice..."
  sleep 5
fi

# Check if Celery workers are running
if ! docker ps | grep -q celery_worker; then
  echo "Iniciando servicios de Celery..."
  docker-compose up -d celery_worker celery_beat
fi

# Activate virtual environment
echo "Activando el entorno virtual tg_env..."
source tg_env/bin/activate

# Set environment variables for local development
# IMPORTANT: These must match docker-compose.yml settings
export PG_HOST=localhost
export PG_PORT=5432
export PG_DATABASE=crypto_db
export PG_USER=bot
export PG_PASSWORD=bot1234

export MONGO_HOST=localhost
export MONGO_PORT=27017
export MONGO_USER=bot  # Must match MONGO_INITDB_ROOT_USERNAME in docker-compose
export MONGO_PASSWORD=bot1234  # Must match MONGO_INITDB_ROOT_PASSWORD in docker-compose
export MONGO_AUTH_SOURCE=admin
export MONGO_DB=tgbot_db

export REDIS_HOST=localhost
export REDIS_PORT=6379

# Initialize database
echo "Setting up database..."
python scripts/setup_database.py

# Start the Telegram bot
echo "Iniciando el bot de Telegram localmente..."
python scripts/telegram_monitor.py & 
BOT_PID=$!
echo "Telegram bot started with PID: $BOT_PID"

# Wait for the bot to finish (or until interrupted)
wait $BOT_PID

# If wait is interrupted, cleanup will be called by the trap

```

### Dockerfile
```text
FROM python:3.10-slim

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    python3-dev \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy project files
COPY . .

# Set Python path
ENV PYTHONPATH=/app

# Default command
CMD ["python", "scripts/app.py"]

```

### requirements.txt
```text
telethon==1.36.0
psycopg2-binary==2.9.9
requests==2.32.3
google-generativeai==0.7.2
python-dotenv==1.0.1
celery==5.3.4
redis==4.6.0
pymongo==4.5.0

```

### __init__.py
```python

```

## scripts Files

### scripts/telegram_monitor.py
```python
import asyncio
import random
import re
import sys
from pathlib import Path
import time
# Add the project root to Python's path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from telethon import TelegramClient, events
import os
import logging
import requests
from datetime import datetime

# Import from new config files
from config.settings import TELEGRAM_API_ID, TELEGRAM_API_HASH, SESSION_PATH, TELEGRAM_PHONE
from config.groups import TELEGRAM_GROUPS
from config.regex_patterns import CALL_PATTERN, RE_CA_BSC_ETH, RE_CA_SOL, DEX_LINK_REGEX, TINYASTRO_REGEX
from config.logging import configure_logging

# Import existing utils
from utils.api_clients import get_pairs_data, get_pair_by_address, parse_float
from utils.db_postgres import connect_postgres, insert_group, insert_message, insert_token, insert_call, update_token_info

# Setup logging
logger = configure_logging()

# Create client using imported settings
logger.info(f"üîß Configurando TelegramClient con API_ID={TELEGRAM_API_ID}")
logger.info(f"üìÇ Usando ruta de sesi√≥n: {SESSION_PATH}")
client = TelegramClient(SESSION_PATH, TELEGRAM_API_ID, TELEGRAM_API_HASH)

# After creating the client, configure connection parameters
client = TelegramClient(SESSION_PATH, TELEGRAM_API_ID, TELEGRAM_API_HASH)

# Adjust connection parameters
client.flood_sleep_threshold = 60  # Higher threshold for flood wait errors
client.connection_retries = 10     # Number of retries per connection attempt
client.retry_delay = 1            # Initial delay between retries

# DexScreener API base URL
DEXSCREENER_BASE_URL = "https://api.dexscreener.com/latest/dex"

# Use imported groups
groups = TELEGRAM_GROUPS

# Rest of your script remains mostly unchanged...
# Just remove any regex pattern definitions since they're now imported
processed_msg_ids = set()

def select_best_pair(pairs):
    """
    Select the best pair from a list of pairs for a token based on liquidity.
    Returns the selected pair object.
    """
    if not pairs:
        return None
    
    # Sort by liquidity (descending) and return highest
    sorted_pairs = sorted(
        pairs,
        key=lambda x: float(x.get('liquidity', {}).get('usd', 0) or 0),
        reverse=True
    )
    
    if sorted_pairs:
        return sorted_pairs[0]
    
    return None

def check_add_best_pair_column():
    """Ensure the best_pair_address column exists in tokens table."""
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return False
            
        cursor = conn.cursor()
        
        # Check if column exists
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'tokens' AND column_name = 'best_pair_address';
        """)
        
        if not cursor.fetchone():
            logger.info("Adding best_pair_address column to tokens table")
            cursor.execute("""
                ALTER TABLE tokens ADD COLUMN best_pair_address character varying(66);
            """)
            conn.commit()
            logger.info("‚úÖ Added best_pair_address column to tokens table")
        
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Error checking/adding column: {e}")
        return False

def update_token_best_pair(token_id, pair_address):
    """Update the best pair address for a token in the tokens table."""
    try:
        conn = connect_postgres()
        if not conn:
            return False
            
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE tokens
            SET best_pair_address = %s
            WHERE token_id = %s
            AND (best_pair_address IS NULL OR best_pair_address != %s)
        """, (pair_address, token_id, pair_address))
        
        updated = cursor.rowcount > 0
        conn.commit()
        cursor.close()
        conn.close()
        
        if updated:
            logger.info(f"‚úÖ Updated best pair address for token {token_id} to {pair_address}")
        
        return updated
    except Exception as e:
        logger.error(f"Error updating token best pair: {e}")
        return False

def insert_price_metrics(token_id, pair):
    """Insert initial price data for a token."""
    try:
        conn = connect_postgres()
        if not conn:
            return False
            
        cursor = conn.cursor()
        
        # Extract data from pair according to the table schema
        pair_address = pair.get('pairAddress', '')
        price_native = pair.get('priceNative', 0)
        price_usd = pair.get('priceUsd', 0)
        txns_buys = pair.get('txns', {}).get('h24', {}).get('buys', 0)
        txns_sells = pair.get('txns', {}).get('h24', {}).get('sells', 0)
        volume = pair.get('volume', {}).get('h24', 0)
        liquidity_base = pair.get('liquidity', {}).get('base', 0)
        liquidity_quote = pair.get('liquidity', {}).get('quote', 0)
        liquidity_usd = pair.get('liquidity', {}).get('usd', 0)
        fdv = pair.get('fdv', 0)
        market_cap = pair.get('marketCap', 0)
        
        # Insert price data
        cursor.execute("""
            INSERT INTO price_metrics
                (token_id, pair_address, timestamp, price_native, price_usd,
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
                liquidity_usd, fdv, market_cap)
            VALUES
                (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, price_native, price_usd,
            txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap
        ))
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info(f"‚úÖ Inserted initial price data for token {token_id}")
        return True
    except Exception as e:
        logger.error(f"Error inserting initial price data: {e}")
        return False

def process_pair_data(token_id, contract_address, blockchain, group_name, pair, message_id, timestamp):
    """Procesa los datos de un par de tokens y actualiza la base de datos"""
    try:
        # Calcular edad del token si es posible
        token_age = None
        if 'pairCreatedAt' in pair:
            creation_time = datetime.fromtimestamp(pair['pairCreatedAt'] / 1000)
            current_time = datetime.now()
            token_age = (current_time - creation_time).days
        
        # Obtener liquidez
        liquidity = None
        if 'liquidity' in pair and 'usd' in pair['liquidity']:
            liquidity = float(pair['liquidity']['usd'] or 0)
        
        # Obtener precio
        call_price = float(pair.get('priceUsd', 0) or 0)
        
        # Actualizar informaci√≥n del token
        success = update_token_info(
            token_id=token_id,
            name=pair.get('baseToken', {}).get('name', 'Unknown'),
            ticker=pair.get('baseToken', {}).get('symbol', 'UNKNOWN'),
            liquidity=liquidity,
            price=call_price,
            dex=pair.get('dexId', 'Unknown'),
            supply=float(pair.get('liquidity', {}).get('base', 0) or 0),
            age=token_age,
            group_name=group_name,  # This stores the group name as text in group_call column
            dexscreener_url=f"https://dexscreener.com/{blockchain}/{contract_address}"
        )
        
        # Log detailed token information
        logger.info(f"üîç Token data: Name={pair.get('baseToken', {}).get('name', 'Unknown')}, "
                  f"Price=${call_price}, Liquidity=${liquidity or 'N/A'}, Age={token_age or 'N/A'}")
        
        if not success:
            logger.warning(f"‚ö†Ô∏è No se pudo actualizar el token {contract_address}")
            return False
        
        # Registrar el call
        try:
            call_id = insert_call(token_id, message_id, timestamp, call_price)
            if not call_id:
                logger.warning(f"‚ö†Ô∏è No se pudo registrar el call para el token {contract_address}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Error espec√≠fico al insertar call: {e}")
            return False
            
        # Save the best pair address if not already set
        pair_address = pair.get('pairAddress')
        if pair_address:
            update_token_best_pair(token_id, pair_address)
            
            # Also insert initial price metrics
            insert_price_metrics(token_id, pair)
        
        logger.info(f"‚úÖ Token {contract_address} procesado exitosamente")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error procesando datos del token {contract_address}: {e}")
        return False

@client.on(events.NewMessage(chats=list(groups.keys())))
async def handle_message(event):
    try:
        # Ensure the best_pair_address column exists
        check_add_best_pair_column()
        
        msg_id = event.message.id
        logger.info(f"üì© Nuevo mensaje ID: {msg_id} en chat: {event.chat_id}")
        if msg_id in processed_msg_ids:
            logger.info(f"üîÑ Mensaje {msg_id} ya procesado, ignorando...")
            return
        processed_msg_ids.add(msg_id)

        chat_id = event.chat_id
        group_name = groups.get(chat_id, "Desconocido")
        text = event.message.message or ""
        timestamp = event.message.date
        sender_id = event.sender_id or 0

        # Detectar si es un "call" basado en el patr√≥n
        is_call = bool(CALL_PATTERN.search(text))

        logger.info(f"üìù Procesando: {group_name} - {text[:50]}{'...' if len(text) > 50 else ''}")

        # Registrar grupo
        try:
            group_id = insert_group(chat_id, group_name)
            logger.info(f"‚úÖ Grupo {group_name} registrado con ID: {group_id}")
        except Exception as e:
            logger.error(f"‚ùå Error al registrar grupo: {e}")
            return

        # Determinar si es una respuesta a otro mensaje
        reply_to = None
        if hasattr(event.message, 'reply_to') and event.message.reply_to:
            reply_to = event.message.reply_to.reply_to_msg_id

        # Guardar mensaje inicialmente (sin token_id a√∫n)
        try:
            message_id = insert_message(
                group_id, 
                timestamp, 
                text, 
                sender_id,
                reply_to=reply_to,
                is_call=is_call
            )
            logger.info(f"üíæ Mensaje guardado con ID: {message_id}")
        except Exception as e:
            logger.error(f"‚ùå Error al guardar mensaje: {e}")
            return

        # Variables para seguimiento de tokens detectados
        token_detected = False
        token_addresses = set()
        detected_token_id = None
        
        # 1. Buscar enlaces de DexScreener primero
        dex_matches = DEX_LINK_REGEX.findall(text)
        for chain, token in dex_matches:
            if token not in token_addresses:
                token_addresses.add(token)
                # Make sure to normalize chain names
                if chain.lower() == "eth":
                    chain = "ethereum"
                logger.info(f"üÜï Token detectado desde DexScreener: {token} ({chain})")
                try:
                    # Insertar token en la base de datos
                    token_id = insert_token(token, chain)
                    detected_token_id = token_id
                    
                    # Obtener datos adicionales desde DexScreener
                    pairs = get_pairs_data(chain, [token])
                    
                    if pairs and len(pairs) > 0:
                        # Select the best pair based on liquidity
                        best_pair = select_best_pair(pairs)
                        
                        # Procesar los datos del token con el mejor par
                        success = process_pair_data(
                            token_id, token, chain, group_name, best_pair, message_id, timestamp
                        )
                        token_detected = success or token_detected
                    else:
                        logger.warning(f"‚ö†Ô∏è No se encontraron pares para el token {token}")
                except Exception as e:
                    logger.error(f"‚ùå Error al procesar token {token} desde DexScreener: {e}")
        
        # Special handling for TinyAstro links (LP addresses)
        tinyastro_matches = TINYASTRO_REGEX.findall(text)
        for lp_address in tinyastro_matches:
            if lp_address not in token_addresses:
                token_addresses.add(lp_address)
                logger.info(f"üÜï LP detectado desde TinyAstro: {lp_address} (solana)")
                
                try:
                    # Insertar LP en la base de datos como token para rastrearlo
                    token_id = insert_token(lp_address, "solana")
                    detected_token_id = token_id
                    
                    # Intentar primero como par, luego como token si falla
                    pairs = get_pair_by_address("solana", lp_address)
                    
                    if pairs and len(pairs) > 0:
                        # Select the best pair (in this case, there should be only one since we're querying by pair address)
                        best_pair = pairs[0]
                        
                        # Extract the actual token address from the pair
                        token_address = best_pair.get('baseToken', {}).get('address')
                        if token_address:
                            logger.info(f"‚úÖ Encontrado token {token_address} desde LP {lp_address}")
                        
                        # Process the pair data
                        success = process_pair_data(
                            token_id, lp_address, "solana", group_name, best_pair, message_id, timestamp
                        )
                        token_detected = success or token_detected
                    else:
                        logger.warning(f"‚ö†Ô∏è No se pudo obtener informaci√≥n para LP {lp_address}")
                except Exception as e:
                    logger.error(f"‚ùå Error al procesar LP {lp_address} desde TinyAstro: {e}")
        
        # First define chains to try for 0x addresses
        ETH_BSC_CHAINS = ["ethereum", "bsc"]  # Try Ethereum first, then BSC

        # Then modify the detection logic:
        # 2. Buscar direcciones de contrato en texto plano
        if not token_detected:
            # Check for 0x addresses (could be ETH or BSC)
            matches = RE_CA_BSC_ETH.findall(text)
            if matches:
                for token in matches:
                    if token in token_addresses:
                        continue
                        
                    token_addresses.add(token)
                    logger.info(f"üÜï Token detectado desde texto: {token} (eth/bsc)")
                    
                    # Try both ETH and BSC chains
                    for chain in ETH_BSC_CHAINS:
                        try:
                            logger.info(f"Intentando chain: {chain} para token {token}")
                            # Get data from DexScreener
                            pairs = get_pairs_data(chain, [token])
                            
                            if pairs and len(pairs) > 0:
                                logger.info(f"‚úÖ Token encontrado en {chain}: {token}")
                                # Select the best pair based on liquidity
                                best_pair = select_best_pair(pairs)
                                
                                # Insert token with correct chain
                                token_id = insert_token(token, chain)
                                detected_token_id = token_id
                                
                                # Process pair data with correct chain
                                success = process_pair_data(
                                    token_id, token, chain, group_name, best_pair, message_id, timestamp
                                )
                                token_detected = success or token_detected
                                if success:
                                    break  # Found the correct chain, no need to try others
                        except Exception as e:
                            logger.error(f"‚ùå Error al procesar token {token} en {chain}: {e}")
        
        # Check for Solana addresses (remains unchanged)
        if not token_detected:
            try:
                matches = RE_CA_SOL.findall(text)
                for token in matches:
                    if token not in token_addresses:
                        token_addresses.add(token)
                        logger.info(f"üÜï Token detectado desde texto: {token} (solana)")
                        
                        try:
                            # Insertar token en la base de datos
                            token_id = insert_token(token, "solana")
                            detected_token_id = token_id
                            
                            # Obtener datos adicionales desde DexScreener
                            pairs = get_pairs_data("solana", [token])
                            
                            if pairs and len(pairs) > 0:
                                # Select the best pair based on liquidity
                                best_pair = select_best_pair(pairs)
                                
                                # Procesar los datos del token
                                success = process_pair_data(
                                    token_id, token, "solana", group_name, best_pair, message_id, timestamp
                                )
                                token_detected = success or token_detected
                                if success:
                                    break  # √âxito, no buscar m√°s tokens
                            else:
                                logger.warning(f"‚ö†Ô∏è No se encontraron pares para el token {token}")
                        except Exception as e:
                            logger.error(f"‚ùå Error al procesar token {token} desde texto: {e}")
            except Exception as e:
                logger.error(f"‚ùå Error al procesar regex para solana: {e}")
        
        # 3. Asegurarnos de que el mensaje est√© actualizado correctamente
        if token_detected and detected_token_id:
            # Verificar si el mensaje ya tiene la marca de call y el token_id
            conn = connect_postgres()
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT is_call, token_id FROM telegram_messages WHERE message_id = %s",
                    (message_id,)
                )
                msg_data = cursor.fetchone()
                
                if not msg_data or not msg_data[0] or msg_data[1] is None:
                    # Si algo no est√° actualizado, hacerlo manualmente
                    cursor.execute(
                        "UPDATE telegram_messages SET is_call = TRUE, token_id = %s WHERE message_id = %s",
                        (detected_token_id, message_id)
                    )
                    conn.commit()
                    logger.info(f"‚úÖ Mensaje {message_id} actualizado como call con token {detected_token_id}")
            except Exception as e:
                logger.error(f"‚ùå Error al verificar/actualizar estado del mensaje: {e}")
                conn.rollback()
            finally:
                conn.close()

        # Final verification to ensure token information was saved
        if token_detected and detected_token_id:
            logger.info(f"‚úì Verification: Token {detected_token_id} processed successfully")
            
            # Double check tokens table record
            conn = connect_postgres()
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT name, ticker, group_call, token_age, best_pair_address FROM tokens WHERE token_id = %s",
                    (detected_token_id,)
                )
                token_data = cursor.fetchone()
                if token_data:
                    logger.info(f"‚úì Token record: Name={token_data[0]}, Ticker={token_data[1]}, "
                              f"Group={token_data[2]}, Age={token_data[3]}, Best Pair={token_data[4]}")
                else:
                    logger.warning("‚ö†Ô∏è Token record not found after processing")
            except Exception as e:
                logger.error(f"‚ùå Error during verification: {e}")
            finally:
                conn.close()

        # Informar resultado final
        if token_detected:
            logger.info(f"üéâ Call registrado exitosamente para mensaje {message_id}")
        else:
            logger.info(f"‚ÑπÔ∏è No se detectaron tokens/calls en este mensaje")
    except Exception as e:
        logger.error(f"‚ùå Error general al procesar mensaje: {e}")
        # Don't re-raise the exception - just log it

async def main():
    """Main function with improved connection handling"""
    # Asegurar que exista el directorio de sesi√≥n
    session_dir = os.path.dirname(SESSION_PATH)
    if not os.path.exists(session_dir):
        os.makedirs(session_dir, exist_ok=True)
        
    logger.info("üöÄ Iniciando el bot de Telegram...")
    
    # Add retry logic
    max_retries = 10
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            await client.start()
            if not await client.is_user_authorized():
                logger.warning("‚ö†Ô∏è Sesi√≥n no autorizada. Autenticaci√≥n requerida.")
                phone = os.getenv("PHONE_NUMBER")
                if phone:
                    await client.send_code_request(phone)
                    code = input("Ingresa el c√≥digo recibido: ")
                    await client.sign_in(phone, code)
                else:
                    logger.error("‚ùå PHONE_NUMBER no definido en .env. Autenticaci√≥n manual requerida.")
                    return
                    
            logger.info("‚úÖ Conexi√≥n a Telegram establecida")
            logger.info(f"üîç Escuchando en {len(groups)} grupos: {list(groups.keys())}")
            
            # Reset retry count on successful connection
            retry_count = 0
            
            # Add a catch-all error handler
            @client.on(events.NewMessage)
            async def error_handler(event):
                try:
                    # Let regular handlers process first
                    await event.continue_propagation()
                except Exception as e:
                    logger.error(f"‚ùå Error no manejado en evento: {e}")
            
            await client.run_until_disconnected()
            
        except (ConnectionError, ServerError) as e:
            retry_count += 1
            wait_time = min(300, (2 ** retry_count) + random.randint(0, 10))  # Exponential backoff with jitter
            logger.warning(f"‚ö†Ô∏è Error de conexi√≥n: {e}. Reintento {retry_count}/{max_retries} en {wait_time} segundos...")
            time.sleep(wait_time)
            
        except Exception as e:
            logger.error(f"‚ùå Error fatal: {e}")
            break
            
    logger.info("üõë Bot detenido y desconectado")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üõë Bot detenido por interrupci√≥n del usuario")
    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}")
    finally:
        logger.info("üîö Fin de ejecuci√≥n")

```

### scripts/setup_database.py
```python
import os
import sys
import logging

# Add parent directory to path so we can import our modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.utils.db_postgres import connect_postgres
from scripts.utils.db_mongo import connect_mongodb

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

def setup_postgres():
    """Initialize PostgreSQL database tables"""
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return False
            
        cursor = conn.cursor()
        
        # Create price_metrics table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS price_metrics (
            token_id BIGINT NOT NULL REFERENCES tokens(token_id),
            pair_address VARCHAR(66),
            timestamp TIMESTAMPTZ NOT NULL,
            price_native NUMERIC,
            price_usd NUMERIC,
            txns_buys INTEGER,
            txns_sells INTEGER,
            volume NUMERIC,
            liquidity_base NUMERIC,
            liquidity_quote NUMERIC,
            liquidity_usd NUMERIC,
            fdv NUMERIC,
            market_cap NUMERIC,
            mongo_id TEXT,
            PRIMARY KEY (token_id, timestamp)
        );
        """)
        
        # Try to convert to TimescaleDB hypertable
        try:
            cursor.execute("""
            SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
            
            CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
            CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
            """)
            logger.info("‚úÖ TimescaleDB hypertable created successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TimescaleDB hypertable creation failed (can be ignored if TimescaleDB not enabled): {e}")
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info("PostgreSQL database setup completed successfully")
        return True
    except Exception as e:
        logger.error(f"Failed to set up PostgreSQL database: {e}")
        if 'conn' in locals() and conn:
            conn.close()
        return False

def test_mongo_connection():
    """Test MongoDB connection with authentication"""
    try:
        client = connect_mongodb()
        if not client:
            return False
            
        db_names = client.list_database_names()
        logger.info(f"MongoDB connection successful. Available databases: {db_names}")
        client.close()
        return True
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        return False

if __name__ == "__main__":
    logger.info("Starting database setup process...")
    
    try:
        # Test MongoDB connection
        logger.info("Testing MongoDB connection...")
        if not test_mongo_connection():
            raise Exception("MongoDB connection failed")
        
        # Setup PostgreSQL database
        logger.info("Setting up PostgreSQL database...")
        if not setup_postgres():
            raise Exception("PostgreSQL setup failed")
            
        logger.info("‚úÖ Database setup completed successfully")
    except Exception as e:
        logger.error(f"‚ùå Database setup failed: {e}")
        sys.exit(1)

```

### scripts/mongo_to_postgres.py
```python
import time
from utils.db_mongo import get_pending_dexscreener_data, mark_as_processed
from utils.db_postgres import connect_postgres

def transfer_dexscreener_data():
    pending_data = get_pending_dexscreener_data("dexscreener_raw")
    if not pending_data:
        print("‚ÑπÔ∏è No hay datos pendientes en MongoDB.")
        return

    conn = connect_postgres()
    cursor = conn.cursor()

    doc_ids = []
    for doc in pending_data:
        token_address = doc.get("token_address")
        blockchain = doc.get("blockchain")
        price_usd = doc.get("priceUsd", "0")

        # Actualizar el token en PostgreSQL (por ahora solo call_price como ejemplo)
        cursor.execute("""
            UPDATE tokens
            SET call_price = %s
            WHERE contract_address = %s AND blockchain = %s
        """, (price_usd, token_address, blockchain))

        doc_ids.append(doc["_id"])

    conn.commit()
    conn.close()

    # Marcar como procesados en MongoDB
    mark_as_processed("dexscreener_raw", doc_ids)
    print(f"‚úÖ Procesados {len(doc_ids)} registros de MongoDB a PostgreSQL.")

if __name__ == "__main__":
    print("üöÄ Iniciando transferencia MongoDB ‚Üí PostgreSQL...")
    while True:
        try:
            transfer_dexscreener_data()
            print("‚è≥ Esperando 5 minutos antes del pr√≥ximo ciclo...")
            time.sleep(300)  # 5 minutos
        except Exception as e:
            print(f"‚ùå Error en transferencia: {e}")
            time.sleep(60)  # Esperar 1 minuto antes de reintentar

```

### scripts/dex_fetcher.py
```python
import time
from utils.db_postgres import connect_postgres
from utils.db_mongo import insert_dexscreener_data
from utils.api_clients import get_pairs_data

def fetch_dexscreener_data():
    conn = connect_postgres()
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT contract_address, blockchain FROM tokens WHERE contract_address IS NOT NULL")
    tokens = cursor.fetchall()
    conn.close()

    for contract_address, blockchain in tokens:
        pairs = get_pairs_data(blockchain, [contract_address])
        if pairs:
            for pair in pairs:
                pair["token_address"] = contract_address
                pair["blockchain"] = blockchain
                pair["fetched_at"] = int(time.time())
                pair["processed"] = False
            insert_dexscreener_data("dexscreener_raw", pairs)
        time.sleep(1)  # Evitar sobrecarga en la API

if __name__ == "__main__":
    print("üöÄ Iniciando fetcher de Dexscreener...")
    while True:
        try:
            fetch_dexscreener_data()
            print("‚è≥ Esperando 5 minutos antes del pr√≥ximo fetch...")
            time.sleep(300)  # 5 minutos
        except Exception as e:
            print(f"‚ùå Error en fetcher: {e}")
            time.sleep(60)  # Esperar 1 minuto antes de reintentar

```

### scripts/gemini_classifier.py
```python
import google.generativeai as genai
from dotenv import load_dotenv
import os

load_dotenv()

def gemini_classify(text, api_key=os.getenv("GEMINI_API_KEY")):
    if not api_key:
        return True
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-pro")
        prompt = f"Clasifica como RELEVANTE s√≥lo si menciona nuevo token:\n'{text}'"
        resp = model.generate_content(prompt)
        return "relevante" in resp.text.strip().lower()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en gemini_classify: {e}. Fallback => True")
        return True

```

### scripts/__init__.py
```python

```

## scripts/price_tracker Files

### scripts/price_tracker/tasks.py
```python
import os
import logging
import requests
from datetime import datetime
from celery import Celery
from scripts.utils.db_mongo import connect_mongodb
from scripts.utils.db_postgres import connect_postgres

# Set up logger
logger = logging.getLogger(__name__)

# DexScreener API base URL
DEXSCREENER_BASE_URL = "https://api.dexscreener.com/latest/dex"

# Create Celery app
app = Celery("price_tracker")

def get_all_tracked_tokens():
    """
    Get all tokens that need price tracking from PostgreSQL.
    Returns token_id, chain, address, and best_pair_address if available.
    """
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return None
            
        cursor = conn.cursor()
        # Get token data including best_pair_address if available
        cursor.execute("""
                SELECT 
                    token_id, blockchain as chain, contract_address as address, best_pair_address
                FROM tokens 
                WHERE contract_address IS NOT NULL
            """)
        
        tokens = cursor.fetchall()
        
        if tokens:
            logger.info(f"‚úÖ Found {len(tokens)} tokens for price tracking")
            if len(tokens) > 2:
                logger.info(f"ü™ô Sample tokens: {tokens[0]}, {tokens[1]}, {tokens[2]}")
        
        cursor.close()
        conn.close()
        return tokens
    except Exception as e:
        logger.error(f"Error fetching tracked tokens: {str(e)}")
        return None

def update_token_best_pair(conn, token_id, pair_address):
    """Update the best pair address for a token in the tokens table."""
    try:
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE tokens
            SET best_pair_address = %s
            WHERE token_id = %s
            AND (best_pair_address IS NULL OR best_pair_address != %s)
        """, (pair_address, token_id, pair_address))
        
        updated = cursor.rowcount > 0
        conn.commit()
        cursor.close()
        
        if updated:
            logger.info(f"‚úÖ Updated best pair address for token {token_id} to {pair_address}")
        
        return updated
    except Exception as e:
        logger.error(f"Error updating token best pair: {str(e)}")
        conn.rollback()
        return False

def select_best_pair(pairs, token_id=None, stored_pair_address=None):
    """
    Select the best pair from a list of pairs for a token.
    
    Priority order:
    1. Use the stored pair address from tokens table if available and still exists
    2. Choose the pair with highest liquidity
    
    Returns the selected pair object.
    """
    if not pairs:
        return None
    
    # Check if we have a stored pair address and if it's in the new data
    if stored_pair_address:
        for pair in pairs:
            if pair.get('pairAddress', '').lower() == stored_pair_address.lower():
                logger.info(f"Using stored pair {stored_pair_address} for token_id {token_id}")
                return pair
    
    # Otherwise sort by liquidity (descending) and return highest
    sorted_pairs = sorted(
        pairs,
        key=lambda x: float(x.get('liquidity', {}).get('usd', 0) or 0),
        reverse=True
    )
    
    if sorted_pairs:
        logger.info(f"Selected new best pair {sorted_pairs[0].get('pairAddress')} for token_id {token_id}")
        return sorted_pairs[0]
    
    return None

def insert_price_metrics(conn, token_id, pair, mongo_id=None):
    """Insert price metrics into PostgreSQL for a given token and pair."""
    if not conn or not pair:
        return False
    
    try:
        cursor = conn.cursor()
        
        # Extract data from pair according to the DB schema
        pair_address = pair.get('pairAddress', '')
        price_native = pair.get('priceNative', 0)
        price_usd = pair.get('priceUsd', 0)
        txns_buys = pair.get('txns', {}).get('h24', {}).get('buys', 0)
        txns_sells = pair.get('txns', {}).get('h24', {}).get('sells', 0)
        volume = pair.get('volume', {}).get('h24', 0)
        liquidity_base = pair.get('liquidity', {}).get('base', 0)
        liquidity_quote = pair.get('liquidity', {}).get('quote', 0)
        liquidity_usd = pair.get('liquidity', {}).get('usd', 0)
        fdv = pair.get('fdv', 0)
        market_cap = pair.get('marketCap', 0)
        
        # Insert the data
        cursor.execute("""
            INSERT INTO price_metrics
                (token_id, pair_address, timestamp, price_native, price_usd,
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
                liquidity_usd, fdv, market_cap, mongo_id)
            VALUES
                (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, price_native, price_usd,
            txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap, mongo_id
        ))
        
        conn.commit()
        cursor.close()
        
        # Also update the best pair address in tokens table if needed
        update_token_best_pair(conn, token_id, pair_address)
        
        return True
    except Exception as e:
        logger.error(f"Error inserting price metrics for token {token_id}: {str(e)}")
        conn.rollback()
        return False

@app.task
def update_all_token_prices():
    """
    Task to fetch token prices from DexScreener API and store in MongoDB/PostgreSQL.
    Uses the best_pair_address from tokens table for consistency.
    """
    try:
        # Get all tracked tokens WITH their best pair addresses
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return "Failed to connect to PostgreSQL"
            
        cursor = conn.cursor()
        cursor.execute("""
            SELECT 
                token_id, blockchain as chain, contract_address as address, best_pair_address
            FROM tokens 
            WHERE contract_address IS NOT NULL
        """)
        
        tokens = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not tokens:
            return "No tokens found for price updates"
        
        logger.info(f"Found {len(tokens)} tokens to update prices")
        
        # Process tokens in batches of 30 (DexScreener's API limit)
        batch_size = 30
        token_batches = []
        
        for i in range(0, len(tokens), batch_size):
            batch = tokens[i:i+batch_size]
            token_batches.append(batch)
        
        # Create mapping of token addresses to token_ids and best pair addresses
        token_info_map = {}
        for token in tokens:
            if token[2]:  # contract_address
                # Store token_id and best_pair_address
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Enhanced token_id_map that includes best_pair_address
        token_info_map = {}
        for token in tokens:
            if token[2]:  # token[2] is contract_address
                # Map structure: address -> (token_id, best_pair_address)
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Process each batch
        documents_stored = 0
        for batch in token_batches:
            # Extract addresses for this batch
            addresses = [token[2] for token in batch if token[2]]
            
            # Make the API request for this batch
            url = f"{DEXSCREENER_BASE_URL}/tokens/{','.join(addresses)}"
            logger.info(f"==> DexScreener tokens batch request: {url}")
            
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                # Store raw response in MongoDB with enhanced token info
                mongo_client = connect_mongodb()
                if mongo_client:
                    collection = mongo_client[os.getenv('MONGO_DB', 'tgbot_db')]['dexscreener_data']
                    mongo_doc = {
                        "raw_data": data,
                        "fetched_at": datetime.now(),
                        "token_addresses": addresses,
                        "token_info_map": token_info_map,  # Store the enhanced mapping
                        "processed": False
                    }
                    result = collection.insert_one(mongo_doc)
                    documents_stored += 1
                    logger.info(f"‚úÖ Stored data in MongoDB with ID: {result.inserted_id}")
                    mongo_client.close()
                
            except Exception as e:
                logger.error(f"Error processing batch: {e}")
                continue
        
        # Trigger the processing task asynchronously
        process_mongodb_data.delay()
        
        return f"Fetched and stored data for {documents_stored} batches in MongoDB"
    except Exception as e:
        logger.error(f"Error in update_all_token_prices: {str(e)}")
        return f"Error: {str(e)}"

@app.task(bind=True, max_retries=3)
def process_mongodb_data(self):
    """
    Task 2: Process unprocessed data from MongoDB and store in PostgreSQL.
    This task performs the actual data transformation and storage.
    """
    try:
        logger.info("üîç Starting process_mongodb_data task")
        
        # Connect to MongoDB
        mongo_client = connect_mongodb()
        if not mongo_client:
            logger.error("Failed to connect to MongoDB")
            return "Failed to connect to MongoDB"
            
        # Get the collection
        db_name = os.getenv('MONGO_DB', 'tgbot_db')
        collection_name = os.getenv('MONGO_COLLECTION_NAME', 'dexscreener_data')
        db = mongo_client[db_name]
        collection = db[collection_name]
        
        # Check unprocessed documents
        total_count = collection.count_documents({})
        unprocessed_count = collection.count_documents({'processed': False})
        
        logger.info(f"üìä Total documents in collection: {total_count}")
        logger.info(f"üìä Unprocessed documents: {unprocessed_count}")
        
        # If no unprocessed documents, just exit
        if unprocessed_count == 0:
            return "No documents to process"
        
        # Find unprocessed documents (limit to 10 at a time for processing chunks)
        unprocessed = collection.find({'processed': False}).limit(10)
        
        tokens_processed = 0
        docs_processed = 0
        processed_token_ids = set()  # Track which token IDs were processed
        
        for doc in unprocessed:
            doc_id = doc.get('_id')
            logger.info(f"üìÑ Processing document ID: {doc_id}")
            
            # Skip documents with no raw_data
            if 'raw_data' not in doc or 'pairs' not in doc['raw_data'] or not doc['raw_data']['pairs']:
                logger.warning(f"Document {doc_id} has no valid pairs data, marking as processed")
                collection.update_one({'_id': doc_id}, {'$set': {'processed': True}})
                docs_processed += 1
                continue
                
            pairs = doc['raw_data']['pairs']
            
            # Get token info map - either from document or rebuild it
            token_info_map = doc.get('token_info_map', {})
            if not token_info_map:
                # Rebuild mapping from token addresses - this is a fallback
                conn = connect_postgres()
                if conn:
                    cursor = conn.cursor()
                    token_info_map = {}
                    
                    # Extract unique base token addresses from pairs
                    base_addresses = set()
                    for pair in pairs:
                        if 'baseToken' in pair and 'address' in pair['baseToken']:
                            base_addresses.add(pair['baseToken']['address'].lower())
                    
                    # Look up token IDs and best pairs for these addresses
                    for address in base_addresses:
                        cursor.execute("""
                            SELECT token_id, best_pair_address 
                            FROM tokens 
                            WHERE LOWER(contract_address) = %s
                        """, (address,))
                        
                        result = cursor.fetchone()
                        if result:
                            token_info_map[address] = {
                                'token_id': result[0],
                                'best_pair_address': result[1]
                            }
                    
                    cursor.close()
                    conn.close()
            
            # Group pairs by token address
            pairs_by_token = {}
            for pair in pairs:
                if 'baseToken' in pair and 'address' in pair['baseToken']:
                    base_address = pair['baseToken']['address'].lower()
                    if base_address not in pairs_by_token:
                        pairs_by_token[base_address] = []
                    pairs_by_token[base_address].append(pair)
            
            # Now process each token's pairs
            conn = connect_postgres()
            if not conn:
                logger.error("Failed to connect to PostgreSQL")
                continue
                
            for token_address, token_pairs in pairs_by_token.items():
                # Skip if we don't have a token_id for this address
                if token_address not in token_info_map:
                    continue
                
                token_info = token_info_map[token_address]
                token_id = token_info['token_id']
                best_pair_address = token_info.get('best_pair_address')
                
                # Select the best pair for this token (using stored pair address if available)
                best_pair = select_best_pair(token_pairs, token_id, best_pair_address)
                
                if best_pair:
                    try:
                        # Insert price data
                        if insert_price_metrics(conn, token_id, best_pair, str(doc_id)):
                            tokens_processed += 1
                            processed_token_ids.add(token_id)
                    except Exception as e:
                        logger.error(f"Error processing token {token_id}: {str(e)}")
                        # Continue to next token even if this one fails
            
            conn.close()
            
            # Mark document as processed
            collection.update_one({'_id': doc_id}, {'$set': {'processed': True}})
            logger.info(f"‚úÖ Marked document {doc_id} as processed")
            docs_processed += 1
        
        mongo_client.close()
        
        # Log summary of which tokens were processed
        logger.info(f"Processed tokens: {sorted(list(processed_token_ids))}")
        
        # If there are more unprocessed documents, trigger another task
        if unprocessed_count > 10:
            process_mongodb_data.delay()
        
        return f"Processed {tokens_processed} tokens from {docs_processed} MongoDB documents"
    except Exception as e:
        logger.error(f"Error in process_mongodb_data: {str(e)}")
        self.retry(countdown=30, exc=e)

@app.task
def process_token_batch(token_batch):
    """
    Process a batch of tokens directly, bypassing MongoDB storage.
    This is for immediate price updates for specific tokens.
    
    token_batch: List of token_id values to update
    """
    try:
        # Connect to PostgreSQL
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return "Failed to connect to PostgreSQL"
        
        # Get token data for this batch
        cursor = conn.cursor()
        placeholders = ','.join(['%s'] * len(token_batch))
        cursor.execute(f"""
            SELECT token_id, blockchain, contract_address, best_pair_address
            FROM tokens
            WHERE token_id IN ({placeholders})
            AND contract_address IS NOT NULL
        """, token_batch)
        
        tokens = cursor.fetchall()
        cursor.close()
        
        if not tokens:
            return f"No valid tokens found in batch"
        
        # Group tokens by batches of 30 for DexScreener API
        addresses = [token[2] for token in tokens if token[2]]
        
        # Create token info mapping
        token_info_map = {}
        for token in tokens:
            if token[2]:  # token[2] is contract_address
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Make batches of 30 addresses
        address_batches = []
        for i in range(0, len(addresses), 30):
            address_batches.append(addresses[i:i+30])
        
        # Process each batch
        processed_tokens = 0
        for address_batch in address_batches:
            url = f"{DEXSCREENER_BASE_URL}/tokens/{','.join(address_batch)}"
            logger.info(f"==> DexScreener direct batch request: {url}")
            
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if 'pairs' not in data or not data['pairs']:
                    continue
                
                pairs = data['pairs']
                
                # Group pairs by token address
                pairs_by_token = {}
                for pair in pairs:
                    if 'baseToken' in pair and 'address' in pair['baseToken']:
                        base_address = pair['baseToken']['address'].lower()
                        if base_address not in pairs_by_token:
                            pairs_by_token[base_address] = []
                        pairs_by_token[base_address].append(pair)
                
                # Process each token's pairs
                for token_address, token_pairs in pairs_by_token.items():
                    # Skip if we don't have token info for this address
                    if token_address not in token_info_map:
                        continue
                    
                    token_info = token_info_map[token_address]
                    token_id = token_info['token_id']
                    best_pair_address = token_info.get('best_pair_address')
                    
                    # Select the best pair for this token
                    best_pair = select_best_pair(token_pairs, token_id, best_pair_address)
                    
                    if best_pair:
                        # Insert price data
                        if insert_price_metrics(conn, token_id, best_pair):
                            processed_tokens += 1
                
            except Exception as e:
                logger.error(f"Error processing direct batch: {e}")
                continue
        
        conn.close()
        return f"Directly processed {processed_tokens} tokens"
    except Exception as e:
        logger.error(f"Error in process_token_batch: {e}")
        return f"Error: {str(e)}"

@app.task
def update_token_batch(token_ids):
    """
    Update prices for a specific batch of tokens by ID.
    This is useful for immediate price updates for specific tokens.
    """
    return process_token_batch(token_ids)

```

### scripts/price_tracker/database.py
```python
import logging
import psycopg2
from scripts.utils.db_postgres import connect_postgres

logger = logging.getLogger(__name__)

def initialize_database():
    """Initialize PostgreSQL database tables if they don't exist."""
    conn = connect_postgres()
    if not conn:
        logger.error("‚ùå Failed to connect to PostgreSQL")
        raise Exception("Could not connect to PostgreSQL database")
        
    try:
        cursor = conn.cursor()
        
        # Create price_metrics table if not exists
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS price_metrics (
            token_id BIGINT NOT NULL REFERENCES tokens(token_id),
            pair_address VARCHAR(66),
            timestamp TIMESTAMPTZ NOT NULL,
            price_native NUMERIC,
            price_usd NUMERIC,
            txns_buys INTEGER,
            txns_sells INTEGER,
            volume NUMERIC,
            liquidity_base NUMERIC,
            liquidity_quote NUMERIC,
            liquidity_usd NUMERIC,
            fdv NUMERIC,
            market_cap NUMERIC,
            mongo_id TEXT,
            PRIMARY KEY (token_id, timestamp)
        );
        """)
        
        # Try to convert to TimescaleDB hypertable
        try:
            cursor.execute("""
            SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
            
            CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
            CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
            """)
            logger.info("‚úÖ TimescaleDB hypertable created successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TimescaleDB hypertable creation failed (can be ignored if TimescaleDB not enabled): {e}")
        
        conn.commit()
        logger.info("‚úÖ PostgreSQL schema initialized successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error initializing schema: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()
import os
import psycopg2
import logging
from contextlib import contextmanager

logger = logging.getLogger(__name__)

@contextmanager
def get_postgres_connection():
    """
    Context manager for getting a PostgreSQL database connection
    """
    conn = None
    try:
        # Get PostgreSQL credentials from environment variables
        db_host = os.getenv('PG_HOST', 'postgres')
        db_port = os.getenv('PG_PORT', '5432')
        db_name = os.getenv('PG_DATABASE', 'crypto_db')
        db_user = os.getenv('PG_USER', 'bot')
        db_password = os.getenv('PG_PASSWORD', 'bot1234')
        
        # Connect to PostgreSQL
        conn = psycopg2.connect(
            host=db_host,
            port=db_port,
            database=db_name,
            user=db_user,
            password=db_password
        )
        
        logger.info("‚úÖ PostgreSQL connection established")
        yield conn
    except Exception as e:
        logger.error(f"‚ùå PostgreSQL connection error: {e}")
        raise
    finally:
        if conn:
            conn.close()

def initialize_database():
    """
    Initialize the PostgreSQL database by creating necessary tables if they don't exist.
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                # Create tokens table if it doesn't exist
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS tokens (
                    token_id SERIAL PRIMARY KEY,
                    token_address VARCHAR(255) NOT NULL,
                    chain_id VARCHAR(50) NOT NULL,
                    symbol VARCHAR(50),
                    name VARCHAR(255),
                    last_price NUMERIC,
                    last_updated TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(token_address, chain_id)
                );
                """)
                
                # Create price_metrics table if it doesn't exist
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS price_metrics (
                    token_id BIGINT NOT NULL REFERENCES tokens(token_id),
                    pair_address VARCHAR(66),
                    timestamp TIMESTAMPTZ NOT NULL,
                    price_native NUMERIC,
                    price_usd NUMERIC,
                    txns_buys INTEGER,
                    txns_sells INTEGER,
                    volume NUMERIC,
                    liquidity_base NUMERIC,
                    liquidity_quote NUMERIC,
                    liquidity_usd NUMERIC,
                    fdv NUMERIC,
                    market_cap NUMERIC,
                    mongo_id TEXT,
                    PRIMARY KEY (token_id, timestamp)
                );
                """)
                
                # Check if TimescaleDB extension is available and create hypertable
                try:
                    # Check if TimescaleDB extension is installed
                    cursor.execute("SELECT extname FROM pg_extension WHERE extname = 'timescaledb';")
                    has_timescaledb = cursor.fetchone() is not None
                    
                    if has_timescaledb:
                        # Convert to TimescaleDB hypertable
                        cursor.execute("""
                        SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
                        """)
                        
                        # Create indices for common queries
                        cursor.execute("""
                        CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
                        """)
                        cursor.execute("""
                        CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
                        """)
                        
                        logger.info("‚úÖ TimescaleDB hypertable and indices created successfully")
                    else:
                        logger.warning("TimescaleDB extension not found. Tables created as regular PostgreSQL tables.")
                except Exception as e:
                    logger.warning(f"TimescaleDB setup error (non-fatal): {e}")
                
                conn.commit()
                logger.info("‚úÖ Database tables initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize database tables: {e}")
        raise

def get_all_tracked_tokens(self=None):
    """
    Get all tracked tokens from the database.
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                SELECT token_id, contract_address, blockchain, name
                FROM tokens
                """)
                
                columns = [desc[0] for desc in cursor.description]
                tokens = [dict(zip(columns, row)) for row in cursor.fetchall()]
                
                return tokens
    except psycopg2.Error as e:
        logger.warning(f"Database error in get_all_tracked_tokens: {e}")
        return []

def insert_price_metrics(price_data):
    """
    Insert price metrics into the price_metrics table.
    
    Args:
        price_data (dict): Dictionary containing price metrics data
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                INSERT INTO price_metrics 
                (token_id, pair_address, timestamp, price_native, price_usd, 
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote, 
                liquidity_usd, fdv, market_cap, mongo_id)
                VALUES 
                (%(token_id)s, %(pair_address)s, %(timestamp)s, %(price_native)s, %(price_usd)s,
                %(txns_buys)s, %(txns_sells)s, %(volume)s, %(liquidity_base)s, %(liquidity_quote)s,
                %(liquidity_usd)s, %(fdv)s, %(market_cap)s, %(mongo_id)s)
                ON CONFLICT (token_id, timestamp) DO UPDATE SET
                price_native = EXCLUDED.price_native,
                price_usd = EXCLUDED.price_usd,
                txns_buys = EXCLUDED.txns_buys,
                txns_sells = EXCLUDED.txns_sells,
                volume = EXCLUDED.volume,
                liquidity_base = EXCLUDED.liquidity_base,
                liquidity_quote = EXCLUDED.liquidity_quote,
                liquidity_usd = EXCLUDED.liquidity_usd,
                fdv = EXCLUDED.fdv,
                market_cap = EXCLUDED.market_cap
                """, price_data)
                
                conn.commit()
                logger.info(f"‚úÖ Price metrics inserted for token_id {price_data.get('token_id')}")
                return True
    except Exception as e:
        logger.error(f"‚ùå Failed to insert price metrics: {e}")
        return False

```

### scripts/price_tracker/mongo_config.py
```python
import os
import logging
from pymongo import MongoClient

logger = logging.getLogger(__name__)

def get_mongodb_connection():
    """Connect to MongoDB with proper authentication."""
    try:
        username = os.getenv("MONGO_USERNAME", "bot")
        password = os.getenv("MONGO_PASSWORD", "bot1234")
        host = os.getenv("MONGO_HOST", "mongo")
        port = os.getenv("MONGO_PORT", "27017")
        
        # Create authenticated connection string
        conn_string = f"mongodb://{username}:{password}@{host}:{port}/admin"
        
        client = MongoClient(conn_string, serverSelectionTimeoutMS=5000)
        
        # Test connection
        client.server_info()
        logger.info(f"‚úÖ Successfully connected to MongoDB at {host}:{port}")
        return client
    except Exception as e:
        logger.error(f"‚ùå Error connecting to MongoDB: {e}")
        raise


```

### scripts/price_tracker/run.py
```python
"""
Helper script to run celery worker and beat.
"""

import os
import sys
from pathlib import Path

# Add project root to Python's path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

if __name__ == "__main__":
    # Import the Celery app
    from scripts.price_tracker.celery_app import app
    
    # Initialize MongoDB
    from utils.db_mongo import initialize_mongodb
    initialize_mongodb()
    
    print("MongoDB initialized. Use these commands to run Celery:")
    print("\nStart worker:")
    print("celery -A scripts.price_tracker.celery_app worker --loglevel=info")
    print("\nStart beat scheduler:")
    print("celery -A scripts.price_tracker.celery_app beat --loglevel=info")

```

## scripts/utils Files

### scripts/utils/project_context.md
```markdown
# TgBot Project Context
Generated on: 2025-03-09 11:01:20

## Project Overview
- **Name**: TgBot
- **Description**: Telegram bot for monitoring cryptocurrency messages and tracking token mentions
**Main Components:**
- Telegram Monitor
- Database Storage
- Price Tracker
- Celery Workers

## Database Schemas

### File: sql/crypto_db_schema.sql
Tables defined:
- `public.telegram_groups`
- `public.telegram_messages`
- `public.token_calls`
- `public.tokens`

## .docker Files

### .docker/config.json
```json
{
  "credsStore": "desktop.exe"
}

```

### .docker/features.json
```json
[Error reading file: [Errno 2] No such file or directory: '/home/marshall/TgBot/.docker/features.json']
```

## config Files

### config/logging.py
```python
"""
Logging configuration
"""
import logging
import sys
from pathlib import Path

# Create logs directory if it doesn't exist
logs_dir = Path(__file__).resolve().parent.parent / "logs"
logs_dir.mkdir(exist_ok=True)

def configure_logging():
    """Configure and return a logger"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(logs_dir / 'telegram_bot.log')
        ]
    )
    return logging.getLogger(__name__)

```

### config/settings.py
```python
"""
Configuration file for application settings
"""
import os
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables
load_dotenv()

# Application root directory
BASE_DIR = Path(__file__).resolve().parent.parent

# Telegram API settings
TELEGRAM_API_ID = int(os.getenv("API_ID", "28644650"))
TELEGRAM_API_HASH = os.getenv("API_HASH", "e963f9b807bcf9d665b1d20de66f7c69")
TELEGRAM_PHONE = os.getenv("PHONE_NUMBER")
SESSION_PATH = os.getenv("SESSION_PATH", "./session/session_name")

# Database settings
DB_CONFIG = {
    "host": os.getenv("PG_HOST", "timescaledb"),
    "port": os.getenv("PG_PORT", "5432"),
    "user": os.getenv("PG_USER", "bot"),
    "password": os.getenv("PG_PASSWORD", "bot1234"),
    "database": os.getenv("PG_DATABASE", "crypto_db")
}

# API Settings
DEXSCREENER_API_TIMEOUT = 15  # seconds


# Add these settings to the existing file

# MongoDB settings
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://mongo:27017/')
MONGO_DB = os.getenv('MONGO_DB', 'tgbot_db')

# Celery settings
CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL', 'redis://redis:6379/0')
CELERY_RESULT_BACKEND = os.getenv('CELERY_RESULT_BACKEND', 'redis://redis:6379/0')

```

### config/groups.py
```python
"""
Configuration file containing Telegram group IDs and names
"""

# Dictionary mapping Telegram group IDs to their readable names
TELEGRAM_GROUPS = {
    -1002124780831: "printoooorrrr gambles",
    -1002161891429: "printoooorrrr calls",
    -1002470555079: "HAVEN (PRV TRAILBLAZER )",
    -1002413987457: "ai ‰ø°Âè∑",
    -1001669758312: "CTü¶Ö zin alpha entries",
    -1001355642881: "CryptoDeus Gems üíéü¶ç",
    -1001756488143: "- SOL -",
    -1001593046999: "VultureCalls",
    -1002390818052: "Obito Calls",
    -1001637465785: "Vulture's Risky Hunter",
    -1001759083119: "$nipers",
    -1001807628627: "Degen Apes",
    -1001870127953: "wat is des",
    -1001710178652: "Obito calls",
    -1001667198684: "Reaper Gems",
    -1001216010763: "Avastar",
    -1001258234189: "1000ùïè Multi Chain",
    -1001539956400: "ApeZone",
    -1002360457432: "MarshallCalls",
    -1002175554746: "SOLANA INSIDER CALLS",
    -1001989363348: "Maythous Degens",
    -1002449259636: "Brokekid Crypto",
    -1002234182572: "Ketchum's Gambles",
    -1001198046393: "Pow's Gem Calls",
    -1001601652682: "Obitos Calls"
}

```

### config/regex_patterns.py
```python
"""
Regular expressions used in the application
"""
import re

# Regex patterns for token and call detection
CALL_PATTERN = re.compile(r"üé≤\s*New\s*Gamble\s*Call", re.IGNORECASE)
RE_CA_BSC_ETH = re.compile(r"\b0x[a-fA-F0-9]{40}\b")
RE_CA_SOL = re.compile(r"\b[1-9A-HJ-NP-Za-km-z]{32,44}\b")
DEX_LINK_REGEX = re.compile(r"https?://dexscreener\.com/(solana|bsc|ethereum)/([^/\s\?]+)")
TINYASTRO_REGEX = re.compile(r"https?://photon-sol\.tinyastro\.io/\w+/lp/([1-9A-HJ-NP-Za-km-z]{32,44})")

```

### config/__init__.py
```python

```

## root Files

### docker-compose.yml
```yaml
services:
  postgres:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_USER=bot
      - POSTGRES_PASSWORD=bot1234
      - POSTGRES_DB=crypto_db
    ports:
      - "5432:5432"
    volumes:
      - ./volumes/postgres-data:/var/lib/postgresql/data
    networks:
      - app-network

  pgadmin:
    image: dpage/pgadmin4
    environment:
      - PGADMIN_DEFAULT_EMAIL=marcelmartino2053@gmail.com
      - PGADMIN_DEFAULT_PASSWORD=admin123
     # Add this to prevent lockouts
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
      - PGADMIN_CONFIG_LOGIN_BANNER="Welcome to PgAdmin"
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - app-network

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - app-network

  mongo:
    image: mongo:latest
    environment:
      - MONGO_INITDB_ROOT_USERNAME=bot  # Match .env MONGO_USER
      - MONGO_INITDB_ROOT_PASSWORD=bot1234  # Match .env MONGO_PASSWORD
    ports:
      - "27017:27017"
    volumes:
      - ./volumes/mongo-data:/data/db
    networks:
      - app-network

  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile.celery
    volumes:
      - ./:/app
    depends_on:
      - postgres
      - redis
      - mongo
    environment:
      - PG_HOST=postgres
      - PG_PORT=5432
      - PG_USER=bot
      - PG_PASSWORD=bot1234
      - PG_DATABASE=crypto_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MONGO_HOST=mongo
      - MONGO_PORT=27017
      - MONGO_USER=bot  # Match .env
      - MONGO_PASSWORD=bot1234
      - MONGO_AUTH_SOURCE=admin
      - MONGO_DB=tgbot_db
      - PYTHONPATH=/app
      - MONGO_COLLECTION_NAME=dexscreener_data
    command: celery -A scripts.price_tracker.celery_app worker --loglevel=info
    networks:
      - app-network

  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile.celery
    volumes:
      - ./:/app
    depends_on:
      - postgres
      - redis
      - mongo
    environment:
      - PG_HOST=postgres
      - PG_PORT=5432
      - PG_USER=bot
      - PG_PASSWORD=bot1234
      - PG_DATABASE=crypto_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MONGO_HOST=mongo
      - MONGO_PORT=27017
      - MONGO_USER=bot
      - MONGO_PASSWORD=bot1234
      - MONGO_COLLECTION_NAME=dexscreener_data
      - PYTHONPATH=/app
    command: celery -A scripts.price_tracker.celery_app beat --loglevel=info
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

```

### start_bot.sh
```bash
#!/bin/bash

# Store PIDs for cleanup
BOT_PID=""

# Function to kill all processes on exit
cleanup() {
    echo "Stopping all processes..."
    
    # Kill the Telegram bot if running
    if [ -n "$BOT_PID" ]; then
        echo "Stopping Telegram bot (PID: $BOT_PID)..."
        kill -TERM $BOT_PID 2>/dev/null || true
    fi
    
    # Stop Celery workers via docker-compose
    echo "Stopping Celery workers..."
    docker-compose stop celery_worker celery_beat
    
    echo "All processes stopped."
    exit 0
}

# Set up signal handling - when Ctrl+C is pressed, run cleanup
trap cleanup SIGINT SIGTERM

# Ensure services are running
echo "Verificando que los servicios est√©n funcionando..."
if ! docker ps | grep -q postgres || ! docker ps | grep -q mongo || ! docker ps | grep -q redis; then
  echo "Iniciando servicios necesarios..."
  docker-compose up -d postgres pgadmin mongo redis
  
  # Give MongoDB time to initialize
  echo "Esperando 5 segundos para que MongoDB se inicialice..."
  sleep 5
fi

# Check if Celery workers are running
if ! docker ps | grep -q celery_worker; then
  echo "Iniciando servicios de Celery..."
  docker-compose up -d celery_worker celery_beat
fi

# Activate virtual environment
echo "Activando el entorno virtual tg_env..."
source tg_env/bin/activate

# Set environment variables for local development
# IMPORTANT: These must match docker-compose.yml settings
export PG_HOST=localhost
export PG_PORT=5432
export PG_DATABASE=crypto_db
export PG_USER=bot
export PG_PASSWORD=bot1234

export MONGO_HOST=localhost
export MONGO_PORT=27017
export MONGO_USER=bot  # Must match MONGO_INITDB_ROOT_USERNAME in docker-compose
export MONGO_PASSWORD=bot1234  # Must match MONGO_INITDB_ROOT_PASSWORD in docker-compose
export MONGO_AUTH_SOURCE=admin
export MONGO_DB=tgbot_db

export REDIS_HOST=localhost
export REDIS_PORT=6379

# Initialize database
echo "Setting up database..."
python scripts/setup_database.py

# Start the Telegram bot
echo "Iniciando el bot de Telegram localmente..."
python scripts/telegram_monitor.py & 
BOT_PID=$!
echo "Telegram bot started with PID: $BOT_PID"

# Wait for the bot to finish (or until interrupted)
wait $BOT_PID

# If wait is interrupted, cleanup will be called by the trap

```

### Dockerfile
```text
FROM python:3.10-slim

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    python3-dev \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy project files
COPY . .

# Set Python path
ENV PYTHONPATH=/app

# Default command
CMD ["python", "scripts/app.py"]

```

### requirements.txt
```text
telethon==1.36.0
psycopg2-binary==2.9.9
requests==2.32.3
google-generativeai==0.7.2
python-dotenv==1.0.1
celery==5.3.4
redis==4.6.0
pymongo==4.5.0

```

### __init__.py
```python

```

## scripts Files

### scripts/telegram_monitor.py
```python
import asyncio
import random
import re
import sys
from pathlib import Path
import time
# Add the project root to Python's path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from telethon import TelegramClient, events
import os
import logging
import requests
from datetime import datetime

# Import from new config files
from config.settings import TELEGRAM_API_ID, TELEGRAM_API_HASH, SESSION_PATH, TELEGRAM_PHONE
from config.groups import TELEGRAM_GROUPS
from config.regex_patterns import CALL_PATTERN, RE_CA_BSC_ETH, RE_CA_SOL, DEX_LINK_REGEX, TINYASTRO_REGEX
from config.logging import configure_logging

# Import existing utils
from utils.api_clients import get_pairs_data, get_pair_by_address, parse_float
from utils.db_postgres import connect_postgres, insert_group, insert_message, insert_token, insert_call, update_token_info

# Setup logging
logger = configure_logging()

# Create client using imported settings
logger.info(f"üîß Configurando TelegramClient con API_ID={TELEGRAM_API_ID}")
logger.info(f"üìÇ Usando ruta de sesi√≥n: {SESSION_PATH}")
client = TelegramClient(SESSION_PATH, TELEGRAM_API_ID, TELEGRAM_API_HASH)

# After creating the client, configure connection parameters
client = TelegramClient(SESSION_PATH, TELEGRAM_API_ID, TELEGRAM_API_HASH)

# Adjust connection parameters
client.flood_sleep_threshold = 60  # Higher threshold for flood wait errors
client.connection_retries = 10     # Number of retries per connection attempt
client.retry_delay = 1            # Initial delay between retries

# DexScreener API base URL
DEXSCREENER_BASE_URL = "https://api.dexscreener.com/latest/dex"

# Use imported groups
groups = TELEGRAM_GROUPS

# Rest of your script remains mostly unchanged...
# Just remove any regex pattern definitions since they're now imported
processed_msg_ids = set()

def select_best_pair(pairs):
    """
    Select the best pair from a list of pairs for a token based on liquidity.
    Returns the selected pair object.
    """
    if not pairs:
        return None
    
    # Sort by liquidity (descending) and return highest
    sorted_pairs = sorted(
        pairs,
        key=lambda x: float(x.get('liquidity', {}).get('usd', 0) or 0),
        reverse=True
    )
    
    if sorted_pairs:
        return sorted_pairs[0]
    
    return None

def check_add_best_pair_column():
    """Ensure the best_pair_address column exists in tokens table."""
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return False
            
        cursor = conn.cursor()
        
        # Check if column exists
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'tokens' AND column_name = 'best_pair_address';
        """)
        
        if not cursor.fetchone():
            logger.info("Adding best_pair_address column to tokens table")
            cursor.execute("""
                ALTER TABLE tokens ADD COLUMN best_pair_address character varying(66);
            """)
            conn.commit()
            logger.info("‚úÖ Added best_pair_address column to tokens table")
        
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Error checking/adding column: {e}")
        return False

def update_token_best_pair(token_id, pair_address):
    """Update the best pair address for a token in the tokens table."""
    try:
        conn = connect_postgres()
        if not conn:
            return False
            
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE tokens
            SET best_pair_address = %s
            WHERE token_id = %s
            AND (best_pair_address IS NULL OR best_pair_address != %s)
        """, (pair_address, token_id, pair_address))
        
        updated = cursor.rowcount > 0
        conn.commit()
        cursor.close()
        conn.close()
        
        if updated:
            logger.info(f"‚úÖ Updated best pair address for token {token_id} to {pair_address}")
        
        return updated
    except Exception as e:
        logger.error(f"Error updating token best pair: {e}")
        return False

def insert_price_metrics(token_id, pair):
    """Insert initial price data for a token."""
    try:
        conn = connect_postgres()
        if not conn:
            return False
            
        cursor = conn.cursor()
        
        # Extract data from pair according to the table schema
        pair_address = pair.get('pairAddress', '')
        price_native = pair.get('priceNative', 0)
        price_usd = pair.get('priceUsd', 0)
        txns_buys = pair.get('txns', {}).get('h24', {}).get('buys', 0)
        txns_sells = pair.get('txns', {}).get('h24', {}).get('sells', 0)
        volume = pair.get('volume', {}).get('h24', 0)
        liquidity_base = pair.get('liquidity', {}).get('base', 0)
        liquidity_quote = pair.get('liquidity', {}).get('quote', 0)
        liquidity_usd = pair.get('liquidity', {}).get('usd', 0)
        fdv = pair.get('fdv', 0)
        market_cap = pair.get('marketCap', 0)
        
        # Insert price data
        cursor.execute("""
            INSERT INTO price_metrics
                (token_id, pair_address, timestamp, price_native, price_usd,
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
                liquidity_usd, fdv, market_cap)
            VALUES
                (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, price_native, price_usd,
            txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap
        ))
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info(f"‚úÖ Inserted initial price data for token {token_id}")
        return True
    except Exception as e:
        logger.error(f"Error inserting initial price data: {e}")
        return False

def process_pair_data(token_id, contract_address, blockchain, group_name, pair, message_id, timestamp):
    """Procesa los datos de un par de tokens y actualiza la base de datos"""
    try:
        # Calcular edad del token si es posible
        token_age = None
        if 'pairCreatedAt' in pair:
            creation_time = datetime.fromtimestamp(pair['pairCreatedAt'] / 1000)
            current_time = datetime.now()
            token_age = (current_time - creation_time).days
        
        # Obtener liquidez
        liquidity = None
        if 'liquidity' in pair and 'usd' in pair['liquidity']:
            liquidity = float(pair['liquidity']['usd'] or 0)
        
        # Obtener precio
        call_price = float(pair.get('priceUsd', 0) or 0)
        
        # Actualizar informaci√≥n del token
        success = update_token_info(
            token_id=token_id,
            name=pair.get('baseToken', {}).get('name', 'Unknown'),
            ticker=pair.get('baseToken', {}).get('symbol', 'UNKNOWN'),
            liquidity=liquidity,
            price=call_price,
            dex=pair.get('dexId', 'Unknown'),
            supply=float(pair.get('liquidity', {}).get('base', 0) or 0),
            age=token_age,
            group_name=group_name,  # This stores the group name as text in group_call column
            dexscreener_url=f"https://dexscreener.com/{blockchain}/{contract_address}"
        )
        
        # Log detailed token information
        logger.info(f"üîç Token data: Name={pair.get('baseToken', {}).get('name', 'Unknown')}, "
                  f"Price=${call_price}, Liquidity=${liquidity or 'N/A'}, Age={token_age or 'N/A'}")
        
        if not success:
            logger.warning(f"‚ö†Ô∏è No se pudo actualizar el token {contract_address}")
            return False
        
        # Registrar el call
        try:
            call_id = insert_call(token_id, message_id, timestamp, call_price)
            if not call_id:
                logger.warning(f"‚ö†Ô∏è No se pudo registrar el call para el token {contract_address}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Error espec√≠fico al insertar call: {e}")
            return False
            
        # Save the best pair address if not already set
        pair_address = pair.get('pairAddress')
        if pair_address:
            update_token_best_pair(token_id, pair_address)
            
            # Also insert initial price metrics
            insert_price_metrics(token_id, pair)
        
        logger.info(f"‚úÖ Token {contract_address} procesado exitosamente")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error procesando datos del token {contract_address}: {e}")
        return False

@client.on(events.NewMessage(chats=list(groups.keys())))
async def handle_message(event):
    try:
        # Ensure the best_pair_address column exists
        check_add_best_pair_column()
        
        msg_id = event.message.id
        logger.info(f"üì© Nuevo mensaje ID: {msg_id} en chat: {event.chat_id}")
        if msg_id in processed_msg_ids:
            logger.info(f"üîÑ Mensaje {msg_id} ya procesado, ignorando...")
            return
        processed_msg_ids.add(msg_id)

        chat_id = event.chat_id
        group_name = groups.get(chat_id, "Desconocido")
        text = event.message.message or ""
        timestamp = event.message.date
        sender_id = event.sender_id or 0

        # Detectar si es un "call" basado en el patr√≥n
        is_call = bool(CALL_PATTERN.search(text))

        logger.info(f"üìù Procesando: {group_name} - {text[:50]}{'...' if len(text) > 50 else ''}")

        # Registrar grupo
        try:
            group_id = insert_group(chat_id, group_name)
            logger.info(f"‚úÖ Grupo {group_name} registrado con ID: {group_id}")
        except Exception as e:
            logger.error(f"‚ùå Error al registrar grupo: {e}")
            return

        # Determinar si es una respuesta a otro mensaje
        reply_to = None
        if hasattr(event.message, 'reply_to') and event.message.reply_to:
            reply_to = event.message.reply_to.reply_to_msg_id

        # Guardar mensaje inicialmente (sin token_id a√∫n)
        try:
            message_id = insert_message(
                group_id, 
                timestamp, 
                text, 
                sender_id,
                reply_to=reply_to,
                is_call=is_call
            )
            logger.info(f"üíæ Mensaje guardado con ID: {message_id}")
        except Exception as e:
            logger.error(f"‚ùå Error al guardar mensaje: {e}")
            return

        # Variables para seguimiento de tokens detectados
        token_detected = False
        token_addresses = set()
        detected_token_id = None
        
        # 1. Buscar enlaces de DexScreener primero
        dex_matches = DEX_LINK_REGEX.findall(text)
        for chain, token in dex_matches:
            if token not in token_addresses:
                token_addresses.add(token)
                # Make sure to normalize chain names
                if chain.lower() == "eth":
                    chain = "ethereum"
                logger.info(f"üÜï Token detectado desde DexScreener: {token} ({chain})")
                try:
                    # Insertar token en la base de datos
                    token_id = insert_token(token, chain)
                    detected_token_id = token_id
                    
                    # Obtener datos adicionales desde DexScreener
                    pairs = get_pairs_data(chain, [token])
                    
                    if pairs and len(pairs) > 0:
                        # Select the best pair based on liquidity
                        best_pair = select_best_pair(pairs)
                        
                        # Procesar los datos del token con el mejor par
                        success = process_pair_data(
                            token_id, token, chain, group_name, best_pair, message_id, timestamp
                        )
                        token_detected = success or token_detected
                    else:
                        logger.warning(f"‚ö†Ô∏è No se encontraron pares para el token {token}")
                except Exception as e:
                    logger.error(f"‚ùå Error al procesar token {token} desde DexScreener: {e}")
        
        # Special handling for TinyAstro links (LP addresses)
        tinyastro_matches = TINYASTRO_REGEX.findall(text)
        for lp_address in tinyastro_matches:
            if lp_address not in token_addresses:
                token_addresses.add(lp_address)
                logger.info(f"üÜï LP detectado desde TinyAstro: {lp_address} (solana)")
                
                try:
                    # Insertar LP en la base de datos como token para rastrearlo
                    token_id = insert_token(lp_address, "solana")
                    detected_token_id = token_id
                    
                    # Intentar primero como par, luego como token si falla
                    pairs = get_pair_by_address("solana", lp_address)
                    
                    if pairs and len(pairs) > 0:
                        # Select the best pair (in this case, there should be only one since we're querying by pair address)
                        best_pair = pairs[0]
                        
                        # Extract the actual token address from the pair
                        token_address = best_pair.get('baseToken', {}).get('address')
                        if token_address:
                            logger.info(f"‚úÖ Encontrado token {token_address} desde LP {lp_address}")
                        
                        # Process the pair data
                        success = process_pair_data(
                            token_id, lp_address, "solana", group_name, best_pair, message_id, timestamp
                        )
                        token_detected = success or token_detected
                    else:
                        logger.warning(f"‚ö†Ô∏è No se pudo obtener informaci√≥n para LP {lp_address}")
                except Exception as e:
                    logger.error(f"‚ùå Error al procesar LP {lp_address} desde TinyAstro: {e}")
        
        # First define chains to try for 0x addresses
        ETH_BSC_CHAINS = ["ethereum", "bsc"]  # Try Ethereum first, then BSC

        # Then modify the detection logic:
        # 2. Buscar direcciones de contrato en texto plano
        if not token_detected:
            # Check for 0x addresses (could be ETH or BSC)
            matches = RE_CA_BSC_ETH.findall(text)
            if matches:
                for token in matches:
                    if token in token_addresses:
                        continue
                        
                    token_addresses.add(token)
                    logger.info(f"üÜï Token detectado desde texto: {token} (eth/bsc)")
                    
                    # Try both ETH and BSC chains
                    for chain in ETH_BSC_CHAINS:
                        try:
                            logger.info(f"Intentando chain: {chain} para token {token}")
                            # Get data from DexScreener
                            pairs = get_pairs_data(chain, [token])
                            
                            if pairs and len(pairs) > 0:
                                logger.info(f"‚úÖ Token encontrado en {chain}: {token}")
                                # Select the best pair based on liquidity
                                best_pair = select_best_pair(pairs)
                                
                                # Insert token with correct chain
                                token_id = insert_token(token, chain)
                                detected_token_id = token_id
                                
                                # Process pair data with correct chain
                                success = process_pair_data(
                                    token_id, token, chain, group_name, best_pair, message_id, timestamp
                                )
                                token_detected = success or token_detected
                                if success:
                                    break  # Found the correct chain, no need to try others
                        except Exception as e:
                            logger.error(f"‚ùå Error al procesar token {token} en {chain}: {e}")
        
        # Check for Solana addresses (remains unchanged)
        if not token_detected:
            try:
                matches = RE_CA_SOL.findall(text)
                for token in matches:
                    if token not in token_addresses:
                        token_addresses.add(token)
                        logger.info(f"üÜï Token detectado desde texto: {token} (solana)")
                        
                        try:
                            # Insertar token en la base de datos
                            token_id = insert_token(token, "solana")
                            detected_token_id = token_id
                            
                            # Obtener datos adicionales desde DexScreener
                            pairs = get_pairs_data("solana", [token])
                            
                            if pairs and len(pairs) > 0:
                                # Select the best pair based on liquidity
                                best_pair = select_best_pair(pairs)
                                
                                # Procesar los datos del token
                                success = process_pair_data(
                                    token_id, token, "solana", group_name, best_pair, message_id, timestamp
                                )
                                token_detected = success or token_detected
                                if success:
                                    break  # √âxito, no buscar m√°s tokens
                            else:
                                logger.warning(f"‚ö†Ô∏è No se encontraron pares para el token {token}")
                        except Exception as e:
                            logger.error(f"‚ùå Error al procesar token {token} desde texto: {e}")
            except Exception as e:
                logger.error(f"‚ùå Error al procesar regex para solana: {e}")
        
        # 3. Asegurarnos de que el mensaje est√© actualizado correctamente
        if token_detected and detected_token_id:
            # Verificar si el mensaje ya tiene la marca de call y el token_id
            conn = connect_postgres()
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT is_call, token_id FROM telegram_messages WHERE message_id = %s",
                    (message_id,)
                )
                msg_data = cursor.fetchone()
                
                if not msg_data or not msg_data[0] or msg_data[1] is None:
                    # Si algo no est√° actualizado, hacerlo manualmente
                    cursor.execute(
                        "UPDATE telegram_messages SET is_call = TRUE, token_id = %s WHERE message_id = %s",
                        (detected_token_id, message_id)
                    )
                    conn.commit()
                    logger.info(f"‚úÖ Mensaje {message_id} actualizado como call con token {detected_token_id}")
            except Exception as e:
                logger.error(f"‚ùå Error al verificar/actualizar estado del mensaje: {e}")
                conn.rollback()
            finally:
                conn.close()

        # Final verification to ensure token information was saved
        if token_detected and detected_token_id:
            logger.info(f"‚úì Verification: Token {detected_token_id} processed successfully")
            
            # Double check tokens table record
            conn = connect_postgres()
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT name, ticker, group_call, token_age, best_pair_address FROM tokens WHERE token_id = %s",
                    (detected_token_id,)
                )
                token_data = cursor.fetchone()
                if token_data:
                    logger.info(f"‚úì Token record: Name={token_data[0]}, Ticker={token_data[1]}, "
                              f"Group={token_data[2]}, Age={token_data[3]}, Best Pair={token_data[4]}")
                else:
                    logger.warning("‚ö†Ô∏è Token record not found after processing")
            except Exception as e:
                logger.error(f"‚ùå Error during verification: {e}")
            finally:
                conn.close()

        # Informar resultado final
        if token_detected:
            logger.info(f"üéâ Call registrado exitosamente para mensaje {message_id}")
        else:
            logger.info(f"‚ÑπÔ∏è No se detectaron tokens/calls en este mensaje")
    except Exception as e:
        logger.error(f"‚ùå Error general al procesar mensaje: {e}")
        # Don't re-raise the exception - just log it

async def main():
    """Main function with improved connection handling"""
    # Asegurar que exista el directorio de sesi√≥n
    session_dir = os.path.dirname(SESSION_PATH)
    if not os.path.exists(session_dir):
        os.makedirs(session_dir, exist_ok=True)
        
    logger.info("üöÄ Iniciando el bot de Telegram...")
    
    # Add retry logic
    max_retries = 10
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            await client.start()
            if not await client.is_user_authorized():
                logger.warning("‚ö†Ô∏è Sesi√≥n no autorizada. Autenticaci√≥n requerida.")
                phone = os.getenv("PHONE_NUMBER")
                if phone:
                    await client.send_code_request(phone)
                    code = input("Ingresa el c√≥digo recibido: ")
                    await client.sign_in(phone, code)
                else:
                    logger.error("‚ùå PHONE_NUMBER no definido en .env. Autenticaci√≥n manual requerida.")
                    return
                    
            logger.info("‚úÖ Conexi√≥n a Telegram establecida")
            logger.info(f"üîç Escuchando en {len(groups)} grupos: {list(groups.keys())}")
            
            # Reset retry count on successful connection
            retry_count = 0
            
            # Add a catch-all error handler
            @client.on(events.NewMessage)
            async def error_handler(event):
                try:
                    # Let regular handlers process first
                    await event.continue_propagation()
                except Exception as e:
                    logger.error(f"‚ùå Error no manejado en evento: {e}")
            
            await client.run_until_disconnected()
            
        except (ConnectionError, ServerError) as e:
            retry_count += 1
            wait_time = min(300, (2 ** retry_count) + random.randint(0, 10))  # Exponential backoff with jitter
            logger.warning(f"‚ö†Ô∏è Error de conexi√≥n: {e}. Reintento {retry_count}/{max_retries} en {wait_time} segundos...")
            time.sleep(wait_time)
            
        except Exception as e:
            logger.error(f"‚ùå Error fatal: {e}")
            break
            
    logger.info("üõë Bot detenido y desconectado")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üõë Bot detenido por interrupci√≥n del usuario")
    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}")
    finally:
        logger.info("üîö Fin de ejecuci√≥n")

```

### scripts/setup_database.py
```python
import os
import sys
import logging

# Add parent directory to path so we can import our modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.utils.db_postgres import connect_postgres
from scripts.utils.db_mongo import connect_mongodb

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

def setup_postgres():
    """Initialize PostgreSQL database tables"""
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return False
            
        cursor = conn.cursor()
        
        # Create price_metrics table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS price_metrics (
            token_id BIGINT NOT NULL REFERENCES tokens(token_id),
            pair_address VARCHAR(66),
            timestamp TIMESTAMPTZ NOT NULL,
            price_native NUMERIC,
            price_usd NUMERIC,
            txns_buys INTEGER,
            txns_sells INTEGER,
            volume NUMERIC,
            liquidity_base NUMERIC,
            liquidity_quote NUMERIC,
            liquidity_usd NUMERIC,
            fdv NUMERIC,
            market_cap NUMERIC,
            mongo_id TEXT,
            PRIMARY KEY (token_id, timestamp)
        );
        """)
        
        # Try to convert to TimescaleDB hypertable
        try:
            cursor.execute("""
            SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
            
            CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
            CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
            """)
            logger.info("‚úÖ TimescaleDB hypertable created successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TimescaleDB hypertable creation failed (can be ignored if TimescaleDB not enabled): {e}")
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info("PostgreSQL database setup completed successfully")
        return True
    except Exception as e:
        logger.error(f"Failed to set up PostgreSQL database: {e}")
        if 'conn' in locals() and conn:
            conn.close()
        return False

def test_mongo_connection():
    """Test MongoDB connection with authentication"""
    try:
        client = connect_mongodb()
        if not client:
            return False
            
        db_names = client.list_database_names()
        logger.info(f"MongoDB connection successful. Available databases: {db_names}")
        client.close()
        return True
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        return False

if __name__ == "__main__":
    logger.info("Starting database setup process...")
    
    try:
        # Test MongoDB connection
        logger.info("Testing MongoDB connection...")
        if not test_mongo_connection():
            raise Exception("MongoDB connection failed")
        
        # Setup PostgreSQL database
        logger.info("Setting up PostgreSQL database...")
        if not setup_postgres():
            raise Exception("PostgreSQL setup failed")
            
        logger.info("‚úÖ Database setup completed successfully")
    except Exception as e:
        logger.error(f"‚ùå Database setup failed: {e}")
        sys.exit(1)

```

### scripts/mongo_to_postgres.py
```python
import time
from utils.db_mongo import get_pending_dexscreener_data, mark_as_processed
from utils.db_postgres import connect_postgres

def transfer_dexscreener_data():
    pending_data = get_pending_dexscreener_data("dexscreener_raw")
    if not pending_data:
        print("‚ÑπÔ∏è No hay datos pendientes en MongoDB.")
        return

    conn = connect_postgres()
    cursor = conn.cursor()

    doc_ids = []
    for doc in pending_data:
        token_address = doc.get("token_address")
        blockchain = doc.get("blockchain")
        price_usd = doc.get("priceUsd", "0")

        # Actualizar el token en PostgreSQL (por ahora solo call_price como ejemplo)
        cursor.execute("""
            UPDATE tokens
            SET call_price = %s
            WHERE contract_address = %s AND blockchain = %s
        """, (price_usd, token_address, blockchain))

        doc_ids.append(doc["_id"])

    conn.commit()
    conn.close()

    # Marcar como procesados en MongoDB
    mark_as_processed("dexscreener_raw", doc_ids)
    print(f"‚úÖ Procesados {len(doc_ids)} registros de MongoDB a PostgreSQL.")

if __name__ == "__main__":
    print("üöÄ Iniciando transferencia MongoDB ‚Üí PostgreSQL...")
    while True:
        try:
            transfer_dexscreener_data()
            print("‚è≥ Esperando 5 minutos antes del pr√≥ximo ciclo...")
            time.sleep(300)  # 5 minutos
        except Exception as e:
            print(f"‚ùå Error en transferencia: {e}")
            time.sleep(60)  # Esperar 1 minuto antes de reintentar

```

### scripts/dex_fetcher.py
```python
import time
from utils.db_postgres import connect_postgres
from utils.db_mongo import insert_dexscreener_data
from utils.api_clients import get_pairs_data

def fetch_dexscreener_data():
    conn = connect_postgres()
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT contract_address, blockchain FROM tokens WHERE contract_address IS NOT NULL")
    tokens = cursor.fetchall()
    conn.close()

    for contract_address, blockchain in tokens:
        pairs = get_pairs_data(blockchain, [contract_address])
        if pairs:
            for pair in pairs:
                pair["token_address"] = contract_address
                pair["blockchain"] = blockchain
                pair["fetched_at"] = int(time.time())
                pair["processed"] = False
            insert_dexscreener_data("dexscreener_raw", pairs)
        time.sleep(1)  # Evitar sobrecarga en la API

if __name__ == "__main__":
    print("üöÄ Iniciando fetcher de Dexscreener...")
    while True:
        try:
            fetch_dexscreener_data()
            print("‚è≥ Esperando 5 minutos antes del pr√≥ximo fetch...")
            time.sleep(300)  # 5 minutos
        except Exception as e:
            print(f"‚ùå Error en fetcher: {e}")
            time.sleep(60)  # Esperar 1 minuto antes de reintentar

```

### scripts/gemini_classifier.py
```python
import google.generativeai as genai
from dotenv import load_dotenv
import os

load_dotenv()

def gemini_classify(text, api_key=os.getenv("GEMINI_API_KEY")):
    if not api_key:
        return True
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-pro")
        prompt = f"Clasifica como RELEVANTE s√≥lo si menciona nuevo token:\n'{text}'"
        resp = model.generate_content(prompt)
        return "relevante" in resp.text.strip().lower()
    except Exception as e:
        print(f"‚ö†Ô∏è Error en gemini_classify: {e}. Fallback => True")
        return True

```

### scripts/__init__.py
```python

```

## scripts/price_tracker Files

### scripts/price_tracker/tasks.py
```python
import os
import logging
import requests
from datetime import datetime
from celery import Celery
from scripts.utils.db_mongo import connect_mongodb
from scripts.utils.db_postgres import connect_postgres

# Set up logger
logger = logging.getLogger(__name__)

# DexScreener API base URL
DEXSCREENER_BASE_URL = "https://api.dexscreener.com/latest/dex"

# Create Celery app
app = Celery("price_tracker")

def get_all_tracked_tokens():
    """
    Get all tokens that need price tracking from PostgreSQL.
    Returns token_id, chain, address, and best_pair_address if available.
    """
    try:
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return None
            
        cursor = conn.cursor()
        # Get token data including best_pair_address if available
        cursor.execute("""
                SELECT 
                    token_id, blockchain as chain, contract_address as address, best_pair_address
                FROM tokens 
                WHERE contract_address IS NOT NULL
            """)
        
        tokens = cursor.fetchall()
        
        if tokens:
            logger.info(f"‚úÖ Found {len(tokens)} tokens for price tracking")
            if len(tokens) > 2:
                logger.info(f"ü™ô Sample tokens: {tokens[0]}, {tokens[1]}, {tokens[2]}")
        
        cursor.close()
        conn.close()
        return tokens
    except Exception as e:
        logger.error(f"Error fetching tracked tokens: {str(e)}")
        return None

def update_token_best_pair(conn, token_id, pair_address):
    """Update the best pair address for a token in the tokens table."""
    try:
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE tokens
            SET best_pair_address = %s
            WHERE token_id = %s
            AND (best_pair_address IS NULL OR best_pair_address != %s)
        """, (pair_address, token_id, pair_address))
        
        updated = cursor.rowcount > 0
        conn.commit()
        cursor.close()
        
        if updated:
            logger.info(f"‚úÖ Updated best pair address for token {token_id} to {pair_address}")
        
        return updated
    except Exception as e:
        logger.error(f"Error updating token best pair: {str(e)}")
        conn.rollback()
        return False

def select_best_pair(pairs, token_id=None, stored_pair_address=None):
    """
    Select the best pair from a list of pairs for a token.
    
    Priority order:
    1. Use the stored pair address from tokens table if available and still exists
    2. Choose the pair with highest liquidity
    
    Returns the selected pair object.
    """
    if not pairs:
        return None
    
    # Check if we have a stored pair address and if it's in the new data
    if stored_pair_address:
        for pair in pairs:
            if pair.get('pairAddress', '').lower() == stored_pair_address.lower():
                logger.info(f"Using stored pair {stored_pair_address} for token_id {token_id}")
                return pair
    
    # Otherwise sort by liquidity (descending) and return highest
    sorted_pairs = sorted(
        pairs,
        key=lambda x: float(x.get('liquidity', {}).get('usd', 0) or 0),
        reverse=True
    )
    
    if sorted_pairs:
        logger.info(f"Selected new best pair {sorted_pairs[0].get('pairAddress')} for token_id {token_id}")
        return sorted_pairs[0]
    
    return None

def insert_price_metrics(conn, token_id, pair, mongo_id=None):
    """Insert price metrics into PostgreSQL for a given token and pair."""
    if not conn or not pair:
        return False
    
    try:
        cursor = conn.cursor()
        
        # Extract data from pair according to the DB schema
        pair_address = pair.get('pairAddress', '')
        price_native = pair.get('priceNative', 0)
        price_usd = pair.get('priceUsd', 0)
        txns_buys = pair.get('txns', {}).get('h24', {}).get('buys', 0)
        txns_sells = pair.get('txns', {}).get('h24', {}).get('sells', 0)
        volume = pair.get('volume', {}).get('h24', 0)
        liquidity_base = pair.get('liquidity', {}).get('base', 0)
        liquidity_quote = pair.get('liquidity', {}).get('quote', 0)
        liquidity_usd = pair.get('liquidity', {}).get('usd', 0)
        fdv = pair.get('fdv', 0)
        market_cap = pair.get('marketCap', 0)
        
        # Insert the data
        cursor.execute("""
            INSERT INTO price_metrics
                (token_id, pair_address, timestamp, price_native, price_usd,
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
                liquidity_usd, fdv, market_cap, mongo_id)
            VALUES
                (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, price_native, price_usd,
            txns_buys, txns_sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap, mongo_id
        ))
        
        conn.commit()
        cursor.close()
        
        # Also update the best pair address in tokens table if needed
        update_token_best_pair(conn, token_id, pair_address)
        
        return True
    except Exception as e:
        logger.error(f"Error inserting price metrics for token {token_id}: {str(e)}")
        conn.rollback()
        return False

@app.task
def update_all_token_prices():
    """
    Task to fetch token prices from DexScreener API and store in MongoDB/PostgreSQL.
    Uses the best_pair_address from tokens table for consistency.
    """
    try:
        # Get all tracked tokens WITH their best pair addresses
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return "Failed to connect to PostgreSQL"
            
        cursor = conn.cursor()
        cursor.execute("""
            SELECT 
                token_id, blockchain as chain, contract_address as address, best_pair_address
            FROM tokens 
            WHERE contract_address IS NOT NULL
        """)
        
        tokens = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not tokens:
            return "No tokens found for price updates"
        
        logger.info(f"Found {len(tokens)} tokens to update prices")
        
        # Process tokens in batches of 30 (DexScreener's API limit)
        batch_size = 30
        token_batches = []
        
        for i in range(0, len(tokens), batch_size):
            batch = tokens[i:i+batch_size]
            token_batches.append(batch)
        
        # Create mapping of token addresses to token_ids and best pair addresses
        token_info_map = {}
        for token in tokens:
            if token[2]:  # contract_address
                # Store token_id and best_pair_address
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Enhanced token_id_map that includes best_pair_address
        token_info_map = {}
        for token in tokens:
            if token[2]:  # token[2] is contract_address
                # Map structure: address -> (token_id, best_pair_address)
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Process each batch
        documents_stored = 0
        for batch in token_batches:
            # Extract addresses for this batch
            addresses = [token[2] for token in batch if token[2]]
            
            # Make the API request for this batch
            url = f"{DEXSCREENER_BASE_URL}/tokens/{','.join(addresses)}"
            logger.info(f"==> DexScreener tokens batch request: {url}")
            
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                # Store raw response in MongoDB with enhanced token info
                mongo_client = connect_mongodb()
                if mongo_client:
                    collection = mongo_client[os.getenv('MONGO_DB', 'tgbot_db')]['dexscreener_data']
                    mongo_doc = {
                        "raw_data": data,
                        "fetched_at": datetime.now(),
                        "token_addresses": addresses,
                        "token_info_map": token_info_map,  # Store the enhanced mapping
                        "processed": False
                    }
                    result = collection.insert_one(mongo_doc)
                    documents_stored += 1
                    logger.info(f"‚úÖ Stored data in MongoDB with ID: {result.inserted_id}")
                    mongo_client.close()
                
            except Exception as e:
                logger.error(f"Error processing batch: {e}")
                continue
        
        # Trigger the processing task asynchronously
        process_mongodb_data.delay()
        
        return f"Fetched and stored data for {documents_stored} batches in MongoDB"
    except Exception as e:
        logger.error(f"Error in update_all_token_prices: {str(e)}")
        return f"Error: {str(e)}"

@app.task(bind=True, max_retries=3)
def process_mongodb_data(self):
    """
    Task 2: Process unprocessed data from MongoDB and store in PostgreSQL.
    This task performs the actual data transformation and storage.
    """
    try:
        logger.info("üîç Starting process_mongodb_data task")
        
        # Connect to MongoDB
        mongo_client = connect_mongodb()
        if not mongo_client:
            logger.error("Failed to connect to MongoDB")
            return "Failed to connect to MongoDB"
            
        # Get the collection
        db_name = os.getenv('MONGO_DB', 'tgbot_db')
        collection_name = os.getenv('MONGO_COLLECTION_NAME', 'dexscreener_data')
        db = mongo_client[db_name]
        collection = db[collection_name]
        
        # Check unprocessed documents
        total_count = collection.count_documents({})
        unprocessed_count = collection.count_documents({'processed': False})
        
        logger.info(f"üìä Total documents in collection: {total_count}")
        logger.info(f"üìä Unprocessed documents: {unprocessed_count}")
        
        # If no unprocessed documents, just exit
        if unprocessed_count == 0:
            return "No documents to process"
        
        # Find unprocessed documents (limit to 10 at a time for processing chunks)
        unprocessed = collection.find({'processed': False}).limit(10)
        
        tokens_processed = 0
        docs_processed = 0
        processed_token_ids = set()  # Track which token IDs were processed
        
        for doc in unprocessed:
            doc_id = doc.get('_id')
            logger.info(f"üìÑ Processing document ID: {doc_id}")
            
            # Skip documents with no raw_data
            if 'raw_data' not in doc or 'pairs' not in doc['raw_data'] or not doc['raw_data']['pairs']:
                logger.warning(f"Document {doc_id} has no valid pairs data, marking as processed")
                collection.update_one({'_id': doc_id}, {'$set': {'processed': True}})
                docs_processed += 1
                continue
                
            pairs = doc['raw_data']['pairs']
            
            # Get token info map - either from document or rebuild it
            token_info_map = doc.get('token_info_map', {})
            if not token_info_map:
                # Rebuild mapping from token addresses - this is a fallback
                conn = connect_postgres()
                if conn:
                    cursor = conn.cursor()
                    token_info_map = {}
                    
                    # Extract unique base token addresses from pairs
                    base_addresses = set()
                    for pair in pairs:
                        if 'baseToken' in pair and 'address' in pair['baseToken']:
                            base_addresses.add(pair['baseToken']['address'].lower())
                    
                    # Look up token IDs and best pairs for these addresses
                    for address in base_addresses:
                        cursor.execute("""
                            SELECT token_id, best_pair_address 
                            FROM tokens 
                            WHERE LOWER(contract_address) = %s
                        """, (address,))
                        
                        result = cursor.fetchone()
                        if result:
                            token_info_map[address] = {
                                'token_id': result[0],
                                'best_pair_address': result[1]
                            }
                    
                    cursor.close()
                    conn.close()
            
            # Group pairs by token address
            pairs_by_token = {}
            for pair in pairs:
                if 'baseToken' in pair and 'address' in pair['baseToken']:
                    base_address = pair['baseToken']['address'].lower()
                    if base_address not in pairs_by_token:
                        pairs_by_token[base_address] = []
                    pairs_by_token[base_address].append(pair)
            
            # Now process each token's pairs
            conn = connect_postgres()
            if not conn:
                logger.error("Failed to connect to PostgreSQL")
                continue
                
            for token_address, token_pairs in pairs_by_token.items():
                # Skip if we don't have a token_id for this address
                if token_address not in token_info_map:
                    continue
                
                token_info = token_info_map[token_address]
                token_id = token_info['token_id']
                best_pair_address = token_info.get('best_pair_address')
                
                # Select the best pair for this token (using stored pair address if available)
                best_pair = select_best_pair(token_pairs, token_id, best_pair_address)
                
                if best_pair:
                    try:
                        # Insert price data
                        if insert_price_metrics(conn, token_id, best_pair, str(doc_id)):
                            tokens_processed += 1
                            processed_token_ids.add(token_id)
                    except Exception as e:
                        logger.error(f"Error processing token {token_id}: {str(e)}")
                        # Continue to next token even if this one fails
            
            conn.close()
            
            # Mark document as processed
            collection.update_one({'_id': doc_id}, {'$set': {'processed': True}})
            logger.info(f"‚úÖ Marked document {doc_id} as processed")
            docs_processed += 1
        
        mongo_client.close()
        
        # Log summary of which tokens were processed
        logger.info(f"Processed tokens: {sorted(list(processed_token_ids))}")
        
        # If there are more unprocessed documents, trigger another task
        if unprocessed_count > 10:
            process_mongodb_data.delay()
        
        return f"Processed {tokens_processed} tokens from {docs_processed} MongoDB documents"
    except Exception as e:
        logger.error(f"Error in process_mongodb_data: {str(e)}")
        self.retry(countdown=30, exc=e)

@app.task
def process_token_batch(token_batch):
    """
    Process a batch of tokens directly, bypassing MongoDB storage.
    This is for immediate price updates for specific tokens.
    
    token_batch: List of token_id values to update
    """
    try:
        # Connect to PostgreSQL
        conn = connect_postgres()
        if not conn:
            logger.error("Failed to connect to PostgreSQL")
            return "Failed to connect to PostgreSQL"
        
        # Get token data for this batch
        cursor = conn.cursor()
        placeholders = ','.join(['%s'] * len(token_batch))
        cursor.execute(f"""
            SELECT token_id, blockchain, contract_address, best_pair_address
            FROM tokens
            WHERE token_id IN ({placeholders})
            AND contract_address IS NOT NULL
        """, token_batch)
        
        tokens = cursor.fetchall()
        cursor.close()
        
        if not tokens:
            return f"No valid tokens found in batch"
        
        # Group tokens by batches of 30 for DexScreener API
        addresses = [token[2] for token in tokens if token[2]]
        
        # Create token info mapping
        token_info_map = {}
        for token in tokens:
            if token[2]:  # token[2] is contract_address
                token_info_map[token[2].lower()] = {
                    'token_id': token[0],
                    'best_pair_address': token[3] if len(token) > 3 else None
                }
        
        # Make batches of 30 addresses
        address_batches = []
        for i in range(0, len(addresses), 30):
            address_batches.append(addresses[i:i+30])
        
        # Process each batch
        processed_tokens = 0
        for address_batch in address_batches:
            url = f"{DEXSCREENER_BASE_URL}/tokens/{','.join(address_batch)}"
            logger.info(f"==> DexScreener direct batch request: {url}")
            
            try:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if 'pairs' not in data or not data['pairs']:
                    continue
                
                pairs = data['pairs']
                
                # Group pairs by token address
                pairs_by_token = {}
                for pair in pairs:
                    if 'baseToken' in pair and 'address' in pair['baseToken']:
                        base_address = pair['baseToken']['address'].lower()
                        if base_address not in pairs_by_token:
                            pairs_by_token[base_address] = []
                        pairs_by_token[base_address].append(pair)
                
                # Process each token's pairs
                for token_address, token_pairs in pairs_by_token.items():
                    # Skip if we don't have token info for this address
                    if token_address not in token_info_map:
                        continue
                    
                    token_info = token_info_map[token_address]
                    token_id = token_info['token_id']
                    best_pair_address = token_info.get('best_pair_address')
                    
                    # Select the best pair for this token
                    best_pair = select_best_pair(token_pairs, token_id, best_pair_address)
                    
                    if best_pair:
                        # Insert price data
                        if insert_price_metrics(conn, token_id, best_pair):
                            processed_tokens += 1
                
            except Exception as e:
                logger.error(f"Error processing direct batch: {e}")
                continue
        
        conn.close()
        return f"Directly processed {processed_tokens} tokens"
    except Exception as e:
        logger.error(f"Error in process_token_batch: {e}")
        return f"Error: {str(e)}"

@app.task
def update_token_batch(token_ids):
    """
    Update prices for a specific batch of tokens by ID.
    This is useful for immediate price updates for specific tokens.
    """
    return process_token_batch(token_ids)

```

### scripts/price_tracker/database.py
```python
import logging
import psycopg2
from scripts.utils.db_postgres import connect_postgres

logger = logging.getLogger(__name__)

def initialize_database():
    """Initialize PostgreSQL database tables if they don't exist."""
    conn = connect_postgres()
    if not conn:
        logger.error("‚ùå Failed to connect to PostgreSQL")
        raise Exception("Could not connect to PostgreSQL database")
        
    try:
        cursor = conn.cursor()
        
        # Create price_metrics table if not exists
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS price_metrics (
            token_id BIGINT NOT NULL REFERENCES tokens(token_id),
            pair_address VARCHAR(66),
            timestamp TIMESTAMPTZ NOT NULL,
            price_native NUMERIC,
            price_usd NUMERIC,
            txns_buys INTEGER,
            txns_sells INTEGER,
            volume NUMERIC,
            liquidity_base NUMERIC,
            liquidity_quote NUMERIC,
            liquidity_usd NUMERIC,
            fdv NUMERIC,
            market_cap NUMERIC,
            mongo_id TEXT,
            PRIMARY KEY (token_id, timestamp)
        );
        """)
        
        # Try to convert to TimescaleDB hypertable
        try:
            cursor.execute("""
            SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
            
            CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
            CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
            """)
            logger.info("‚úÖ TimescaleDB hypertable created successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TimescaleDB hypertable creation failed (can be ignored if TimescaleDB not enabled): {e}")
        
        conn.commit()
        logger.info("‚úÖ PostgreSQL schema initialized successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error initializing schema: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()
import os
import psycopg2
import logging
from contextlib import contextmanager

logger = logging.getLogger(__name__)

@contextmanager
def get_postgres_connection():
    """
    Context manager for getting a PostgreSQL database connection
    """
    conn = None
    try:
        # Get PostgreSQL credentials from environment variables
        db_host = os.getenv('PG_HOST', 'postgres')
        db_port = os.getenv('PG_PORT', '5432')
        db_name = os.getenv('PG_DATABASE', 'crypto_db')
        db_user = os.getenv('PG_USER', 'bot')
        db_password = os.getenv('PG_PASSWORD', 'bot1234')
        
        # Connect to PostgreSQL
        conn = psycopg2.connect(
            host=db_host,
            port=db_port,
            database=db_name,
            user=db_user,
            password=db_password
        )
        
        logger.info("‚úÖ PostgreSQL connection established")
        yield conn
    except Exception as e:
        logger.error(f"‚ùå PostgreSQL connection error: {e}")
        raise
    finally:
        if conn:
            conn.close()

def initialize_database():
    """
    Initialize the PostgreSQL database by creating necessary tables if they don't exist.
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                # Create tokens table if it doesn't exist
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS tokens (
                    token_id SERIAL PRIMARY KEY,
                    token_address VARCHAR(255) NOT NULL,
                    chain_id VARCHAR(50) NOT NULL,
                    symbol VARCHAR(50),
                    name VARCHAR(255),
                    last_price NUMERIC,
                    last_updated TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(token_address, chain_id)
                );
                """)
                
                # Create price_metrics table if it doesn't exist
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS price_metrics (
                    token_id BIGINT NOT NULL REFERENCES tokens(token_id),
                    pair_address VARCHAR(66),
                    timestamp TIMESTAMPTZ NOT NULL,
                    price_native NUMERIC,
                    price_usd NUMERIC,
                    txns_buys INTEGER,
                    txns_sells INTEGER,
                    volume NUMERIC,
                    liquidity_base NUMERIC,
                    liquidity_quote NUMERIC,
                    liquidity_usd NUMERIC,
                    fdv NUMERIC,
                    market_cap NUMERIC,
                    mongo_id TEXT,
                    PRIMARY KEY (token_id, timestamp)
                );
                """)
                
                # Check if TimescaleDB extension is available and create hypertable
                try:
                    # Check if TimescaleDB extension is installed
                    cursor.execute("SELECT extname FROM pg_extension WHERE extname = 'timescaledb';")
                    has_timescaledb = cursor.fetchone() is not None
                    
                    if has_timescaledb:
                        # Convert to TimescaleDB hypertable
                        cursor.execute("""
                        SELECT create_hypertable('price_metrics', 'timestamp', if_not_exists => TRUE);
                        """)
                        
                        # Create indices for common queries
                        cursor.execute("""
                        CREATE INDEX IF NOT EXISTS idx_price_metrics_token_id ON price_metrics(token_id);
                        """)
                        cursor.execute("""
                        CREATE INDEX IF NOT EXISTS idx_price_metrics_timestamp ON price_metrics(timestamp DESC);
                        """)
                        
                        logger.info("‚úÖ TimescaleDB hypertable and indices created successfully")
                    else:
                        logger.warning("TimescaleDB extension not found. Tables created as regular PostgreSQL tables.")
                except Exception as e:
                    logger.warning(f"TimescaleDB setup error (non-fatal): {e}")
                
                conn.commit()
                logger.info("‚úÖ Database tables initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize database tables: {e}")
        raise

def get_all_tracked_tokens(self=None):
    """
    Get all tracked tokens from the database.
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                SELECT token_id, contract_address, blockchain, name
                FROM tokens
                """)
                
                columns = [desc[0] for desc in cursor.description]
                tokens = [dict(zip(columns, row)) for row in cursor.fetchall()]
                
                return tokens
    except psycopg2.Error as e:
        logger.warning(f"Database error in get_all_tracked_tokens: {e}")
        return []

def insert_price_metrics(price_data):
    """
    Insert price metrics into the price_metrics table.
    
    Args:
        price_data (dict): Dictionary containing price metrics data
    """
    try:
        with get_postgres_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                INSERT INTO price_metrics 
                (token_id, pair_address, timestamp, price_native, price_usd, 
                txns_buys, txns_sells, volume, liquidity_base, liquidity_quote, 
                liquidity_usd, fdv, market_cap, mongo_id)
                VALUES 
                (%(token_id)s, %(pair_address)s, %(timestamp)s, %(price_native)s, %(price_usd)s,
                %(txns_buys)s, %(txns_sells)s, %(volume)s, %(liquidity_base)s, %(liquidity_quote)s,
                %(liquidity_usd)s, %(fdv)s, %(market_cap)s, %(mongo_id)s)
                ON CONFLICT (token_id, timestamp) DO UPDATE SET
                price_native = EXCLUDED.price_native,
                price_usd = EXCLUDED.price_usd,
                txns_buys = EXCLUDED.txns_buys,
                txns_sells = EXCLUDED.txns_sells,
                volume = EXCLUDED.volume,
                liquidity_base = EXCLUDED.liquidity_base,
                liquidity_quote = EXCLUDED.liquidity_quote,
                liquidity_usd = EXCLUDED.liquidity_usd,
                fdv = EXCLUDED.fdv,
                market_cap = EXCLUDED.market_cap
                """, price_data)
                
                conn.commit()
                logger.info(f"‚úÖ Price metrics inserted for token_id {price_data.get('token_id')}")
                return True
    except Exception as e:
        logger.error(f"‚ùå Failed to insert price metrics: {e}")
        return False

```

### scripts/price_tracker/mongo_config.py
```python
import os
import logging
from pymongo import MongoClient

logger = logging.getLogger(__name__)

def get_mongodb_connection():
    """Connect to MongoDB with proper authentication."""
    try:
        username = os.getenv("MONGO_USERNAME", "bot")
        password = os.getenv("MONGO_PASSWORD", "bot1234")
        host = os.getenv("MONGO_HOST", "mongo")
        port = os.getenv("MONGO_PORT", "27017")
        
        # Create authenticated connection string
        conn_string = f"mongodb://{username}:{password}@{host}:{port}/admin"
        
        client = MongoClient(conn_string, serverSelectionTimeoutMS=5000)
        
        # Test connection
        client.server_info()
        logger.info(f"‚úÖ Successfully connected to MongoDB at {host}:{port}")
        return client
    except Exception as e:
        logger.error(f"‚ùå Error connecting to MongoDB: {e}")
        raise


```

### scripts/price_tracker/run.py
```python
"""
Helper script to run celery worker and beat.
"""

import os
import sys
from pathlib import Path

# Add project root to Python's path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

if __name__ == "__main__":
    # Import the Celery app
    from scripts.price_tracker.celery_app import app
    
    # Initialize MongoDB
    from utils.db_mongo import initialize_mongodb
    initialize_mongodb()
    
    print("MongoDB initialized. Use these commands to run Celery:")
    print("\nStart worker:")
    print("celery -A scripts.price_tracker.celery_app worker --loglevel=info")
    print("\nStart beat scheduler:")
    print("celery -A scripts.price_tracker.celery_app beat --loglevel=info")

```

## scripts/utils Files

### scripts/utils/db_postgres.py
```python
import os
import logging
import psycopg2
from datetime import datetime, timezone

from scripts.utils.api_clients import parse_float

logger = logging.getLogger(__name__)

def connect_postgres():
    """Connect to PostgreSQL using environment variables."""
    try:
        # Use variables that match .env
        host = os.getenv("PG_HOST", "postgres")
        port = os.getenv("PG_PORT", "5432")
        dbname = os.getenv("PG_DATABASE", "crypto_db")
        user = os.getenv("PG_USER", "bot")
        password = os.getenv("PG_PASSWORD", "bot1234")
        
        conn = psycopg2.connect(
            host=host,
            port=port,
            dbname=dbname,
            user=user,
            password=password
        )
        
        logger.info(f"‚úÖ Successfully connected to PostgreSQL at {host}:{port}")
        return conn
    except Exception as e:
        logger.error(f"‚ùå Error connecting to PostgreSQL: {e}")
        return None

def insert_group(telegram_id, name):
    """Inserta un grupo en la base de datos o retorna su ID si ya existe"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        cursor.execute(
            """
            INSERT INTO telegram_groups (telegram_id, name)
            VALUES (%s, %s)
            ON CONFLICT (telegram_id) DO UPDATE
            SET name = EXCLUDED.name
            RETURNING group_id
            """,
            (telegram_id, name)
        )
        group_id = cursor.fetchone()[0]
        conn.commit()
        return group_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_message(group_id, timestamp, text, sender_id, reply_to=None, token_id=None, is_call=False):
    """Inserta un mensaje en la base de datos"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        cursor.execute(
            """
            INSERT INTO telegram_messages (
                group_id, message_timestamp, raw_text, sender_id, 
                reply_to_message_id, token_id, is_call
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            RETURNING message_id
            """,
            (group_id, timestamp, text, sender_id, reply_to, token_id, is_call)
        )
        message_id = cursor.fetchone()[0]
        conn.commit()
        return message_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_token(contract_address, blockchain="ethereum", name=None, ticker=None):
    """Inserta un token en la base de datos o retorna su ID si ya existe"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Primero buscar si el token ya existe
        cursor.execute(
            """
            SELECT token_id FROM tokens 
            WHERE contract_address = %s AND blockchain = %s
            """,
            (contract_address, blockchain)
        )
        existing = cursor.fetchone()
        
        if existing:
            return existing[0]
        
        # Si no existe, insertar nuevo token
        cursor.execute(
            """
            INSERT INTO tokens (
                name, ticker, blockchain, contract_address, 
                supply, call_price
            )
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING token_id
            """,
            (
                name or "Unknown", 
                ticker or "UNKNOWN",
                blockchain,
                contract_address,
                0,  # Default supply
                0   # Default call_price
            )
        )
        token_id = cursor.fetchone()[0]
        conn.commit()
        return token_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_call(token_id, message_id, timestamp, call_price=None):
    """Guarda un 'call' de un token en un mensaje"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Si no se proporciona precio, obtenerlo del token
        if call_price is None:
            cursor.execute(
                "SELECT call_price FROM tokens WHERE token_id = %s", 
                (token_id,)
            )
            token_data = cursor.fetchone()
            if token_data:
                call_price = token_data[0]
            else:
                call_price = 0
        
        # Verificar si ya existe un call para este token y mensaje
        cursor.execute(
            """
            SELECT call_id FROM token_calls 
            WHERE token_id = %s AND message_id = %s
            """, 
            (token_id, message_id)
        )
        existing = cursor.fetchone()
        
        if existing:
            # Actualizar call existente
            call_id = existing[0]
            cursor.execute(
                """
                UPDATE token_calls SET 
                    call_timestamp = %s,
                    call_price = %s
                WHERE call_id = %s
                """, 
                (timestamp, call_price, call_id)
            )
        else:
            # Insertar nuevo call
            cursor.execute(
                """
                INSERT INTO token_calls (
                    token_id, call_timestamp, call_price, message_id
                ) VALUES (
                    %s, %s, %s, %s
                )
                RETURNING call_id
                """, 
                (token_id, timestamp, call_price, message_id)
            )
            call_id = cursor.fetchone()[0]
        
        # Actualizar el mensaje para marcar que contiene un call
        cursor.execute(
            """
            UPDATE telegram_messages 
            SET is_call = TRUE, token_id = %s
            WHERE message_id = %s
            """, 
            (token_id, message_id)
        )
        
        conn.commit()
        logger.info(f"‚úÖ Call registrado con ID: {call_id} para token {token_id} y mensaje {message_id}")
        return call_id
    except Exception as e:
        logger.error(f"‚ùå Error en insert_call: {e}")
        conn.rollback()
        return None
    finally:
        conn.close()

def update_token_info(token_id, name=None, ticker=None, liquidity=None, price=None, 
                     dex=None, supply=None, age=None, group_name=None, dexscreener_url=None):
    """Actualiza la informaci√≥n de un token"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Construir la consulta din√°micamente basada en los par√°metros proporcionados
        updates = []
        params = []
        
        if name:
            updates.append("name = %s")
            params.append(name)
        
        if ticker:
            updates.append("ticker = %s")
            params.append(ticker)
        
        if liquidity is not None:
            updates.append("first_call_liquidity = %s")
            params.append(liquidity)
        
        if price is not None:
            updates.append("call_price = %s")
            params.append(price)
        
        if dex:
            updates.append("dex = %s")
            params.append(dex)
        
        if supply is not None:
            updates.append("supply = %s")
            params.append(supply)
        
        if age is not None:
            updates.append("token_age = %s")
            params.append(age)
        
        if group_name:
            updates.append("group_call = COALESCE(group_call, %s) ")
            params.append(group_name)
            
        if dexscreener_url:
            updates.append("dexscreener_url = %s")
            params.append(dexscreener_url)
        
        if not updates:
            return False
        
        # A√±adir token_id al final de los par√°metros
        params.append(token_id)
        
        # Construir y ejecutar la consulta
        query = f"UPDATE tokens SET {', '.join(updates)} WHERE token_id = %s"
        cursor.execute(query, params)
        
        conn.commit()
        return True
    except Exception as e:
        logger.error(f"‚ùå Error al actualizar token {token_id}: {e}")
        conn.rollback()
        return False
    finally:
        conn.close()

def get_all_tracked_tokens(connection=None):
    """Get all tokens from the tokens table for price tracking."""
    try:
        logger.info("üîç Getting all tracked tokens from database")
        
        # Use connection if provided or create new one
        conn = connection
        if conn is None:
            logger.info("üîå Creating new PostgreSQL connection")
            conn = connect_postgres()
            
        if not conn:
            logger.error("‚ùå Failed to connect to PostgreSQL")
            return []
            
        cursor = conn.cursor()
        
        # Debug: Check table structure
        try:
            cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name='tokens'")
            columns = [col[0] for col in cursor.fetchall()]
            logger.info(f"üìä Tokens table columns: {columns}")
        except Exception as e:
            logger.error(f"‚ùå Error checking table structure: {e}")
        
        # Execute the query with more informative error handling
        try:
            query = """
                SELECT 
                    token_id, blockchain as chain, contract_address as address
                FROM tokens 
                WHERE contract_address IS NOT NULL
            """
            logger.info(f"üîç Executing SQL: {query}")
            cursor.execute(query)
        except Exception as e:
            logger.error(f"‚ùå SQL query error: {e}")
            if connection is None and conn:
                conn.close()
            return []
        
        tokens = cursor.fetchall()
        cursor.close()
        
        if tokens:
            logger.info(f"‚úÖ Found {len(tokens)} tokens for price tracking")
            # Log a sample of tokens
            for i, token in enumerate(tokens[:3]):
                logger.info(f"ü™ô Sample token {i+1}: {token}")
        else:
            logger.warning("‚ö†Ô∏è No tokens found with contract addresses")
            
        return tokens
    except Exception as e:
        logger.error(f"‚ùå Database error in get_all_tracked_tokens: {e}")
        return []

def insert_price_metrics(connection, token_id, pair_data):
    """
    Insert price metrics into the price_metrics table.
    
    Args:
        connection: PostgreSQL connection
        token_id: ID from the tokens table
        pair_data: Dictionary with price data from DexScreener API
    """
    try:
        cursor = connection.cursor()
        
        # Extract values with fallbacks
        pair_address = pair_data.get('pairAddress')
        price_native = pair_data.get('priceNative')
        price_usd = pair_data.get('priceUsd')
        
        # Get transaction counts
        txns = pair_data.get('txns', {}).get('h24', {})
        buys = txns.get('buys', 0)
        sells = txns.get('sells', 0)
        volume = pair_data.get('volume', {}).get('h24')
        
        # Get liquidity data
        liquidity = pair_data.get('liquidity', {})
        liquidity_base = liquidity.get('base')
        liquidity_quote = liquidity.get('quote')
        liquidity_usd = liquidity.get('usd')
        
        # Get other metrics
        fdv = pair_data.get('fdv')
        market_cap = pair_data.get('marketCap')
        
        # MongoDB document ID reference
        mongo_id = str(pair_data.get('_id')) if hasattr(pair_data, 'get') and pair_data.get('_id') else None
        
        # Current timestamp
        timestamp = datetime.now(timezone.utc)
        
        cursor.execute("""
            INSERT INTO price_metrics 
            (token_id, pair_address, timestamp, price_native, price_usd, 
             txns_buys, txns_sells, volume, liquidity_base, liquidity_quote, 
             liquidity_usd, fdv, market_cap, mongo_id)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (token_id, timestamp) 
            DO UPDATE SET
                price_native = EXCLUDED.price_native,
                price_usd = EXCLUDED.price_usd,
                volume = EXCLUDED.volume,
                liquidity_base = EXCLUDED.liquidity_base,
                liquidity_quote = EXCLUDED.liquidity_quote,
                liquidity_usd = EXCLUDED.liquidity_usd,
                fdv = EXCLUDED.fdv,
                market_cap = EXCLUDED.market_cap
        """, (
            token_id, pair_address, timestamp, price_native, price_usd,
            buys, sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap, mongo_id
        ))
        
        connection.commit()
        
        # Also update the price in the tokens table
        if price_usd is not None:
            cursor.execute("""
                UPDATE tokens
                SET call_price = %s
                WHERE token_id = %s
            """, (price_usd, token_id))
            connection.commit()
            
        cursor.close()
        return True
    except Exception as e:
        logger.error(f"‚ùå Error inserting price metrics: {e}")
        if connection:
            connection.rollback()
        return False

def insert_price_metrics(token_id, timestamp, pair, mongo_id=None):
    """
    Insert price metrics into the database from a DexScreener pair object
    """
    try:
        conn = connect_postgres()
        cursor = conn.cursor()
        
        # Extract metrics from pair
        price_native = parse_float(pair.get("priceNative"))
        price_usd = parse_float(pair.get("priceUsd"))
        liquidity_base = parse_float(pair.get("liquidity", {}).get("base"))
        liquidity_quote = parse_float(pair.get("liquidity", {}).get("quote"))
        fdv = parse_float(pair.get("fdv"))
        market_cap = parse_float(pair.get("marketCap"))
        pair_address = pair.get("pairAddress")
        
        # Get transaction counts for last minute
        txns = pair.get("txns", {}).get("m1", {})
        buys = int(txns.get("buys") or 0)
        sells = int(txns.get("sells") or 0)
        volume = parse_float(txns.get("volume"))
        
        # Insert into database
        cursor.execute("""
            INSERT INTO price_metrics 
            (token_id, pair_address, timestamp, price_native, price_usd, 
             txns_m1_buys, txns_m1_sells, volume_m1, 
             liquidity_base, liquidity_quote, fdv, market_cap)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, timestamp, price_native, price_usd,
            buys, sells, volume,
            liquidity_base, liquidity_quote, fdv, market_cap
        ))
        
        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Error inserting price metrics: {e}")
        return False

```

### scripts/utils/db_mongo.py
```python
"""
MongoDB utility functions for the price tracker.
"""

import os
import logging
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

# Configure logging
logger = logging.getLogger(__name__)

def connect_mongodb():
    """Connect to MongoDB with proper authentication."""
    try:
        username = os.getenv("MONGO_USER", "bot")
        password = os.getenv("MONGO_PASSWORD", "bot1234")
        host = os.getenv("MONGO_HOST", "mongo")  # In Docker this will be "mongo", locally "localhost"
        port = os.getenv("MONGO_PORT", "27017")
        
        logger.info(f"Connecting to MongoDB at {host}:{port} with user {username}")
        
        # Build connection string - explicitly specify authSource=admin
        conn_string = f"mongodb://{username}:{password}@{host}:{port}/admin?authSource=admin"
        
        client = MongoClient(conn_string, serverSelectionTimeoutMS=5000)
        
        # Force a command to test the connection
        client.admin.command('ping')
        logger.info(f"‚úÖ Successfully connected to MongoDB at {host}:{port}")
        return client
    except Exception as e:
        logger.error(f"‚ùå Error connecting to MongoDB: {e}")
        return None

# Add compatibility function
def get_mongo_client():
    """Alias for connect_mongodb()"""
    return connect_mongodb()

def get_collection(db_name=None, collection_name=None):
    """Get a specific MongoDB collection."""
    try:
        client = connect_mongodb()
        if not client:
            return None
        
        if not db_name:
            db_name = os.getenv("MONGO_DB", "tgbot_db")
            
        db = client[db_name]
        
        if not collection_name:
            collection_name = os.getenv("MONGO_COLLECTION_NAME", "dexscreener_data")
            
        return db[collection_name]
    except Exception as e:
        logger.error(f"‚ùå Error getting MongoDB collection: {e}")
        return None

def get_dexscreener_collection():
    """Get the collection for DexScreener data."""
    client = connect_mongodb()
    if not client:
        logger.error("Failed to connect to MongoDB")
        return None
    
    db_name = os.getenv("MONGO_DB", "tgbot_db")
    collection_name = os.getenv("MONGO_COLLECTION_NAME", "dexscreener_data")
    
    # Create the database and collection if they don't exist
    db = client[db_name]
    if collection_name not in db.list_collection_names():
        logger.info(f"Creating collection {collection_name} in {db_name}")
        db.create_collection(collection_name)
    
    return db[collection_name]

def initialize_mongodb():
    """Initialize MongoDB collections and indexes."""
    try:
        client = connect_mongodb()
        if not client:
            return False
            
        db_name = os.getenv("MONGO_DB", "tgbot_db")
        db = client[db_name]
        
        # Create or get collection
        dexscreener_collection = db["dexscreener_data"]
        
        # Create indexes
        dexscreener_collection.create_index([("token_address", 1)])
        dexscreener_collection.create_index([("pair_address", 1)])
        dexscreener_collection.create_index([("fetched_at", -1)])
        dexscreener_collection.create_index([("processed", 1)])
        
        logger.info("‚úÖ MongoDB indexes created")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize MongoDB: {e}")
        return False

```

### scripts/utils/api_clients.py
```python
import requests
import logging
from config.settings import DEXSCREENER_API_TIMEOUT

logger = logging.getLogger(__name__)

def get_pairs_data(chain, token_addresses):
    """Get data for token(s) from DexScreener API"""
    if not token_addresses:
        return []
    
    # Use the correct endpoint for tokens
    joined = ",".join(token_addresses)
    url = f"https://api.dexscreener.com/latest/dex/tokens/{joined}"
    logger.info(f"==> DexScreener tokens request: {url}")
    
    try:
        # Use timeout from settings
        r = requests.get(url, timeout=DEXSCREENER_API_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        
        if "pairs" in data and data["pairs"] is not None and len(data["pairs"]) > 0:
            return data["pairs"]
        
        # If no results from token endpoint, try the pair endpoint
        logger.info(f"No token data found, trying pair endpoint...")
        return get_pair_by_address(chain, token_addresses[0])
        
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Error get_pairs_data => {e}")
        return []

def get_pair_by_address(chain, pair_address):
    """Get data for a specific pair from DexScreener API"""
    # Map chain names if needed
    chain_map = {
        "eth": "ethereum",
        "solana": "solana",
        "bsc": "bsc",
        "ethereum": "ethereum"
    }
    
    dex_chain = chain_map.get(chain.lower(), chain.lower())
    
    url = f"https://api.dexscreener.com/latest/dex/pairs/{dex_chain}/{pair_address}"
    logger.info(f"==> DexScreener pairs request: {url}")
    
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        data = r.json()
        
        if "pair" in data and data["pair"] is not None:
            # Return as list for consistency
            return [data["pair"]]
        
        return []
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Error get_pair_by_address => {e}")
        return []

def parse_float(value, default=None):
    """
    Safely parse float values from various sources.
    Handles None, empty strings, and non-numeric strings.
    """
    if value is None:
        return default
    
    if isinstance(value, (float, int)):
        return float(value)
    
    try:
        # Remove common currency formatting
        if isinstance(value, str):
            value = value.replace(',', '')
            value = value.replace('$', '')
            value = value.strip()
            if value == '':
                return default
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Could not convert '{value}' to float")
        return default


```

### scripts/utils/common.py
```python
import logging

logger = logging.getLogger(__name__)

def parse_float(value, default=None):
    """
    Safely parse float values from various sources.
    Handles None, empty strings, and non-numeric strings.
    """
    if value is None:
        return default
    
    if isinstance(value, (float, int)):
        return float(value)
    
    try:
        # Remove common currency formatting
        if isinstance(value, str):
            value = value.replace(',', '')
            value = value.replace('$', '')
            value = value.strip()
            if value == '':
                return default
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Could not convert '{value}' to float")
        return default

```

### scripts/utils/__init__.py
```python

```

## sql Files

### sql/crypto_db_schema.sql
```sql
--
-- PostgreSQL database dump
--

-- Dumped from database version 14.16
-- Dumped by pg_dump version 14.16

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: timescaledb; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS timescaledb WITH SCHEMA public;


--
-- Name: EXTENSION timescaledb; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION timescaledb IS 'Enables scalable inserts and complex queries for time-series data (Community Edition)';


--
-- Name: telegram_groups_group_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.telegram_groups_group_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.telegram_groups_group_id_seq OWNER TO bot;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: telegram_groups; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.telegram_groups (
    group_id bigint DEFAULT nextval('public.telegram_groups_group_id_seq'::regclass) NOT NULL,
    telegram_id bigint NOT NULL,
    name text NOT NULL,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.telegram_groups OWNER TO bot;

--
-- Name: TABLE telegram_groups; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.telegram_groups IS 'Almacena los grupos de Telegram monitoreados por el bot.';


--
-- Name: telegram_messages_message_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.telegram_messages_message_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.telegram_messages_message_id_seq OWNER TO bot;

--
-- Name: telegram_messages; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.telegram_messages (
    message_id bigint DEFAULT nextval('public.telegram_messages_message_id_seq'::regclass) NOT NULL,
    group_id bigint NOT NULL,
    message_timestamp timestamp with time zone NOT NULL,
    raw_text text NOT NULL,
    sender_id bigint NOT NULL,
    is_call boolean DEFAULT false NOT NULL,
    reply_to_message_id bigint,
    token_id bigint,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.telegram_messages OWNER TO bot;

--
-- Name: TABLE telegram_messages; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.telegram_messages IS 'Registra todos los mensajes capturados de los grupos de Telegram.';


--
-- Name: token_calls_call_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.token_calls_call_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.token_calls_call_id_seq OWNER TO bot;

--
-- Name: token_calls; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.token_calls (
    call_id bigint DEFAULT nextval('public.token_calls_call_id_seq'::regclass) NOT NULL,
    token_id bigint NOT NULL,
    call_timestamp timestamp with time zone NOT NULL,
    call_price numeric(30,10) NOT NULL,
    message_id bigint,
    note text,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.token_calls OWNER TO bot;

--
-- Name: TABLE token_calls; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.token_calls IS 'Almacena los calls espec√≠ficos de tokens detectados en mensajes.';


--
-- Name: tokens_token_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.tokens_token_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.tokens_token_id_seq OWNER TO bot;

--
-- Name: tokens; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.tokens (
    token_id bigint DEFAULT nextval('public.tokens_token_id_seq'::regclass) NOT NULL,
    name text NOT NULL,
    ticker text NOT NULL,
    blockchain text NOT NULL,
    contract_address text NOT NULL,
    dex text,
    first_call_liquidity numeric(30,10),
    supply numeric(30,10) NOT NULL,
    initial_call_timestamp timestamp with time zone DEFAULT now(),
    group_call text,
    call_price numeric(30,10) NOT NULL,
    token_age integer,
    dexscreener_url text
);


ALTER TABLE public.tokens OWNER TO bot;

--
-- Name: TABLE tokens; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.tokens IS 'Registra los tokens detectados en los mensajes de Telegram, con soporte para cualquier blockchain.';


--
-- Name: telegram_groups telegram_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_groups
    ADD CONSTRAINT telegram_groups_pkey PRIMARY KEY (group_id);


--
-- Name: telegram_groups telegram_groups_telegram_id_key; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_groups
    ADD CONSTRAINT telegram_groups_telegram_id_key UNIQUE (telegram_id);


--
-- Name: telegram_messages telegram_messages_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_messages
    ADD CONSTRAINT telegram_messages_pkey PRIMARY KEY (message_id);


--
-- Name: token_calls token_calls_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.token_calls
    ADD CONSTRAINT token_calls_pkey PRIMARY KEY (call_id);


--
-- Name: tokens tokens_contract_address_key; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.tokens
    ADD CONSTRAINT tokens_contract_address_key UNIQUE (contract_address);


--
-- Name: tokens tokens_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.tokens
    ADD CONSTRAINT tokens_pkey PRIMARY KEY (token_id);


--
-- Name: idx_telegram_groups_telegram_id; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_groups_telegram_id ON public.telegram_groups USING btree (telegram_id);


--
-- Name: idx_telegram_msg_group; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_msg_group ON public.telegram_messages USING btree (group_id);


--
-- Name: idx_telegram_msg_time; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_msg_time ON public.telegram_messages USING btree (message_timestamp DESC);


--
-- Name: idx_token_calls_time; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_token_calls_time ON public.token_calls USING btree (call_timestamp DESC);


--
-- Name: idx_token_calls_token; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_token_calls_token ON public.token_calls USING btree (token_id);


--
-- Name: idx_tokens_blockchain; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_tokens_blockchain ON public.tokens USING btree (blockchain);


--
-- Name: idx_tokens_contract; Type: INDEX; Schema: public; Owner: bot


... [File truncated, 50 more lines] ...
```

### scripts/utils/db_postgres.py
```python
import os
import logging
import psycopg2
from datetime import datetime, timezone

from scripts.utils.api_clients import parse_float

logger = logging.getLogger(__name__)

def connect_postgres():
    """Connect to PostgreSQL using environment variables."""
    try:
        # Use variables that match .env
        host = os.getenv("PG_HOST", "postgres")
        port = os.getenv("PG_PORT", "5432")
        dbname = os.getenv("PG_DATABASE", "crypto_db")
        user = os.getenv("PG_USER", "bot")
        password = os.getenv("PG_PASSWORD", "bot1234")
        
        conn = psycopg2.connect(
            host=host,
            port=port,
            dbname=dbname,
            user=user,
            password=password
        )
        
        logger.info(f"‚úÖ Successfully connected to PostgreSQL at {host}:{port}")
        return conn
    except Exception as e:
        logger.error(f"‚ùå Error connecting to PostgreSQL: {e}")
        return None

def insert_group(telegram_id, name):
    """Inserta un grupo en la base de datos o retorna su ID si ya existe"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        cursor.execute(
            """
            INSERT INTO telegram_groups (telegram_id, name)
            VALUES (%s, %s)
            ON CONFLICT (telegram_id) DO UPDATE
            SET name = EXCLUDED.name
            RETURNING group_id
            """,
            (telegram_id, name)
        )
        group_id = cursor.fetchone()[0]
        conn.commit()
        return group_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_message(group_id, timestamp, text, sender_id, reply_to=None, token_id=None, is_call=False):
    """Inserta un mensaje en la base de datos"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        cursor.execute(
            """
            INSERT INTO telegram_messages (
                group_id, message_timestamp, raw_text, sender_id, 
                reply_to_message_id, token_id, is_call
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            RETURNING message_id
            """,
            (group_id, timestamp, text, sender_id, reply_to, token_id, is_call)
        )
        message_id = cursor.fetchone()[0]
        conn.commit()
        return message_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_token(contract_address, blockchain="ethereum", name=None, ticker=None):
    """Inserta un token en la base de datos o retorna su ID si ya existe"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Primero buscar si el token ya existe
        cursor.execute(
            """
            SELECT token_id FROM tokens 
            WHERE contract_address = %s AND blockchain = %s
            """,
            (contract_address, blockchain)
        )
        existing = cursor.fetchone()
        
        if existing:
            return existing[0]
        
        # Si no existe, insertar nuevo token
        cursor.execute(
            """
            INSERT INTO tokens (
                name, ticker, blockchain, contract_address, 
                supply, call_price
            )
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING token_id
            """,
            (
                name or "Unknown", 
                ticker or "UNKNOWN",
                blockchain,
                contract_address,
                0,  # Default supply
                0   # Default call_price
            )
        )
        token_id = cursor.fetchone()[0]
        conn.commit()
        return token_id
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        conn.close()

def insert_call(token_id, message_id, timestamp, call_price=None):
    """Guarda un 'call' de un token en un mensaje"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Si no se proporciona precio, obtenerlo del token
        if call_price is None:
            cursor.execute(
                "SELECT call_price FROM tokens WHERE token_id = %s", 
                (token_id,)
            )
            token_data = cursor.fetchone()
            if token_data:
                call_price = token_data[0]
            else:
                call_price = 0
        
        # Verificar si ya existe un call para este token y mensaje
        cursor.execute(
            """
            SELECT call_id FROM token_calls 
            WHERE token_id = %s AND message_id = %s
            """, 
            (token_id, message_id)
        )
        existing = cursor.fetchone()
        
        if existing:
            # Actualizar call existente
            call_id = existing[0]
            cursor.execute(
                """
                UPDATE token_calls SET 
                    call_timestamp = %s,
                    call_price = %s
                WHERE call_id = %s
                """, 
                (timestamp, call_price, call_id)
            )
        else:
            # Insertar nuevo call
            cursor.execute(
                """
                INSERT INTO token_calls (
                    token_id, call_timestamp, call_price, message_id
                ) VALUES (
                    %s, %s, %s, %s
                )
                RETURNING call_id
                """, 
                (token_id, timestamp, call_price, message_id)
            )
            call_id = cursor.fetchone()[0]
        
        # Actualizar el mensaje para marcar que contiene un call
        cursor.execute(
            """
            UPDATE telegram_messages 
            SET is_call = TRUE, token_id = %s
            WHERE message_id = %s
            """, 
            (token_id, message_id)
        )
        
        conn.commit()
        logger.info(f"‚úÖ Call registrado con ID: {call_id} para token {token_id} y mensaje {message_id}")
        return call_id
    except Exception as e:
        logger.error(f"‚ùå Error en insert_call: {e}")
        conn.rollback()
        return None
    finally:
        conn.close()

def update_token_info(token_id, name=None, ticker=None, liquidity=None, price=None, 
                     dex=None, supply=None, age=None, group_name=None, dexscreener_url=None):
    """Actualiza la informaci√≥n de un token"""
    conn = connect_postgres()
    cursor = conn.cursor()
    try:
        # Construir la consulta din√°micamente basada en los par√°metros proporcionados
        updates = []
        params = []
        
        if name:
            updates.append("name = %s")
            params.append(name)
        
        if ticker:
            updates.append("ticker = %s")
            params.append(ticker)
        
        if liquidity is not None:
            updates.append("first_call_liquidity = %s")
            params.append(liquidity)
        
        if price is not None:
            updates.append("call_price = %s")
            params.append(price)
        
        if dex:
            updates.append("dex = %s")
            params.append(dex)
        
        if supply is not None:
            updates.append("supply = %s")
            params.append(supply)
        
        if age is not None:
            updates.append("token_age = %s")
            params.append(age)
        
        if group_name:
            updates.append("group_call = COALESCE(group_call, %s) ")
            params.append(group_name)
            
        if dexscreener_url:
            updates.append("dexscreener_url = %s")
            params.append(dexscreener_url)
        
        if not updates:
            return False
        
        # A√±adir token_id al final de los par√°metros
        params.append(token_id)
        
        # Construir y ejecutar la consulta
        query = f"UPDATE tokens SET {', '.join(updates)} WHERE token_id = %s"
        cursor.execute(query, params)
        
        conn.commit()
        return True
    except Exception as e:
        logger.error(f"‚ùå Error al actualizar token {token_id}: {e}")
        conn.rollback()
        return False
    finally:
        conn.close()

def get_all_tracked_tokens(connection=None):
    """Get all tokens from the tokens table for price tracking."""
    try:
        logger.info("üîç Getting all tracked tokens from database")
        
        # Use connection if provided or create new one
        conn = connection
        if conn is None:
            logger.info("üîå Creating new PostgreSQL connection")
            conn = connect_postgres()
            
        if not conn:
            logger.error("‚ùå Failed to connect to PostgreSQL")
            return []
            
        cursor = conn.cursor()
        
        # Debug: Check table structure
        try:
            cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name='tokens'")
            columns = [col[0] for col in cursor.fetchall()]
            logger.info(f"üìä Tokens table columns: {columns}")
        except Exception as e:
            logger.error(f"‚ùå Error checking table structure: {e}")
        
        # Execute the query with more informative error handling
        try:
            query = """
                SELECT 
                    token_id, blockchain as chain, contract_address as address
                FROM tokens 
                WHERE contract_address IS NOT NULL
            """
            logger.info(f"üîç Executing SQL: {query}")
            cursor.execute(query)
        except Exception as e:
            logger.error(f"‚ùå SQL query error: {e}")
            if connection is None and conn:
                conn.close()
            return []
        
        tokens = cursor.fetchall()
        cursor.close()
        
        if tokens:
            logger.info(f"‚úÖ Found {len(tokens)} tokens for price tracking")
            # Log a sample of tokens
            for i, token in enumerate(tokens[:3]):
                logger.info(f"ü™ô Sample token {i+1}: {token}")
        else:
            logger.warning("‚ö†Ô∏è No tokens found with contract addresses")
            
        return tokens
    except Exception as e:
        logger.error(f"‚ùå Database error in get_all_tracked_tokens: {e}")
        return []

def insert_price_metrics(connection, token_id, pair_data):
    """
    Insert price metrics into the price_metrics table.
    
    Args:
        connection: PostgreSQL connection
        token_id: ID from the tokens table
        pair_data: Dictionary with price data from DexScreener API
    """
    try:
        cursor = connection.cursor()
        
        # Extract values with fallbacks
        pair_address = pair_data.get('pairAddress')
        price_native = pair_data.get('priceNative')
        price_usd = pair_data.get('priceUsd')
        
        # Get transaction counts
        txns = pair_data.get('txns', {}).get('h24', {})
        buys = txns.get('buys', 0)
        sells = txns.get('sells', 0)
        volume = pair_data.get('volume', {}).get('h24')
        
        # Get liquidity data
        liquidity = pair_data.get('liquidity', {})
        liquidity_base = liquidity.get('base')
        liquidity_quote = liquidity.get('quote')
        liquidity_usd = liquidity.get('usd')
        
        # Get other metrics
        fdv = pair_data.get('fdv')
        market_cap = pair_data.get('marketCap')
        
        # MongoDB document ID reference
        mongo_id = str(pair_data.get('_id')) if hasattr(pair_data, 'get') and pair_data.get('_id') else None
        
        # Current timestamp
        timestamp = datetime.now(timezone.utc)
        
        cursor.execute("""
            INSERT INTO price_metrics 
            (token_id, pair_address, timestamp, price_native, price_usd, 
             txns_buys, txns_sells, volume, liquidity_base, liquidity_quote, 
             liquidity_usd, fdv, market_cap, mongo_id)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (token_id, timestamp) 
            DO UPDATE SET
                price_native = EXCLUDED.price_native,
                price_usd = EXCLUDED.price_usd,
                volume = EXCLUDED.volume,
                liquidity_base = EXCLUDED.liquidity_base,
                liquidity_quote = EXCLUDED.liquidity_quote,
                liquidity_usd = EXCLUDED.liquidity_usd,
                fdv = EXCLUDED.fdv,
                market_cap = EXCLUDED.market_cap
        """, (
            token_id, pair_address, timestamp, price_native, price_usd,
            buys, sells, volume, liquidity_base, liquidity_quote,
            liquidity_usd, fdv, market_cap, mongo_id
        ))
        
        connection.commit()
        
        # Also update the price in the tokens table
        if price_usd is not None:
            cursor.execute("""
                UPDATE tokens
                SET call_price = %s
                WHERE token_id = %s
            """, (price_usd, token_id))
            connection.commit()
            
        cursor.close()
        return True
    except Exception as e:
        logger.error(f"‚ùå Error inserting price metrics: {e}")
        if connection:
            connection.rollback()
        return False

def insert_price_metrics(token_id, timestamp, pair, mongo_id=None):
    """
    Insert price metrics into the database from a DexScreener pair object
    """
    try:
        conn = connect_postgres()
        cursor = conn.cursor()
        
        # Extract metrics from pair
        price_native = parse_float(pair.get("priceNative"))
        price_usd = parse_float(pair.get("priceUsd"))
        liquidity_base = parse_float(pair.get("liquidity", {}).get("base"))
        liquidity_quote = parse_float(pair.get("liquidity", {}).get("quote"))
        fdv = parse_float(pair.get("fdv"))
        market_cap = parse_float(pair.get("marketCap"))
        pair_address = pair.get("pairAddress")
        
        # Get transaction counts for last minute
        txns = pair.get("txns", {}).get("m1", {})
        buys = int(txns.get("buys") or 0)
        sells = int(txns.get("sells") or 0)
        volume = parse_float(txns.get("volume"))
        
        # Insert into database
        cursor.execute("""
            INSERT INTO price_metrics 
            (token_id, pair_address, timestamp, price_native, price_usd, 
             txns_m1_buys, txns_m1_sells, volume_m1, 
             liquidity_base, liquidity_quote, fdv, market_cap)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            token_id, pair_address, timestamp, price_native, price_usd,
            buys, sells, volume,
            liquidity_base, liquidity_quote, fdv, market_cap
        ))
        
        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.error(f"Error inserting price metrics: {e}")
        return False

```

### scripts/utils/db_mongo.py
```python
"""
MongoDB utility functions for the price tracker.
"""

import os
import logging
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

# Configure logging
logger = logging.getLogger(__name__)

def connect_mongodb():
    """Connect to MongoDB with proper authentication."""
    try:
        username = os.getenv("MONGO_USER", "bot")
        password = os.getenv("MONGO_PASSWORD", "bot1234")
        host = os.getenv("MONGO_HOST", "mongo")  # In Docker this will be "mongo", locally "localhost"
        port = os.getenv("MONGO_PORT", "27017")
        
        logger.info(f"Connecting to MongoDB at {host}:{port} with user {username}")
        
        # Build connection string - explicitly specify authSource=admin
        conn_string = f"mongodb://{username}:{password}@{host}:{port}/admin?authSource=admin"
        
        client = MongoClient(conn_string, serverSelectionTimeoutMS=5000)
        
        # Force a command to test the connection
        client.admin.command('ping')
        logger.info(f"‚úÖ Successfully connected to MongoDB at {host}:{port}")
        return client
    except Exception as e:
        logger.error(f"‚ùå Error connecting to MongoDB: {e}")
        return None

# Add compatibility function
def get_mongo_client():
    """Alias for connect_mongodb()"""
    return connect_mongodb()

def get_collection(db_name=None, collection_name=None):
    """Get a specific MongoDB collection."""
    try:
        client = connect_mongodb()
        if not client:
            return None
        
        if not db_name:
            db_name = os.getenv("MONGO_DB", "tgbot_db")
            
        db = client[db_name]
        
        if not collection_name:
            collection_name = os.getenv("MONGO_COLLECTION_NAME", "dexscreener_data")
            
        return db[collection_name]
    except Exception as e:
        logger.error(f"‚ùå Error getting MongoDB collection: {e}")
        return None

def get_dexscreener_collection():
    """Get the collection for DexScreener data."""
    client = connect_mongodb()
    if not client:
        logger.error("Failed to connect to MongoDB")
        return None
    
    db_name = os.getenv("MONGO_DB", "tgbot_db")
    collection_name = os.getenv("MONGO_COLLECTION_NAME", "dexscreener_data")
    
    # Create the database and collection if they don't exist
    db = client[db_name]
    if collection_name not in db.list_collection_names():
        logger.info(f"Creating collection {collection_name} in {db_name}")
        db.create_collection(collection_name)
    
    return db[collection_name]

def initialize_mongodb():
    """Initialize MongoDB collections and indexes."""
    try:
        client = connect_mongodb()
        if not client:
            return False
            
        db_name = os.getenv("MONGO_DB", "tgbot_db")
        db = client[db_name]
        
        # Create or get collection
        dexscreener_collection = db["dexscreener_data"]
        
        # Create indexes
        dexscreener_collection.create_index([("token_address", 1)])
        dexscreener_collection.create_index([("pair_address", 1)])
        dexscreener_collection.create_index([("fetched_at", -1)])
        dexscreener_collection.create_index([("processed", 1)])
        
        logger.info("‚úÖ MongoDB indexes created")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize MongoDB: {e}")
        return False

```

### scripts/utils/api_clients.py
```python
import requests
import logging
from config.settings import DEXSCREENER_API_TIMEOUT

logger = logging.getLogger(__name__)

def get_pairs_data(chain, token_addresses):
    """Get data for token(s) from DexScreener API"""
    if not token_addresses:
        return []
    
    # Use the correct endpoint for tokens
    joined = ",".join(token_addresses)
    url = f"https://api.dexscreener.com/latest/dex/tokens/{joined}"
    logger.info(f"==> DexScreener tokens request: {url}")
    
    try:
        # Use timeout from settings
        r = requests.get(url, timeout=DEXSCREENER_API_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        
        if "pairs" in data and data["pairs"] is not None and len(data["pairs"]) > 0:
            return data["pairs"]
        
        # If no results from token endpoint, try the pair endpoint
        logger.info(f"No token data found, trying pair endpoint...")
        return get_pair_by_address(chain, token_addresses[0])
        
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Error get_pairs_data => {e}")
        return []

def get_pair_by_address(chain, pair_address):
    """Get data for a specific pair from DexScreener API"""
    # Map chain names if needed
    chain_map = {
        "eth": "ethereum",
        "solana": "solana",
        "bsc": "bsc",
        "ethereum": "ethereum"
    }
    
    dex_chain = chain_map.get(chain.lower(), chain.lower())
    
    url = f"https://api.dexscreener.com/latest/dex/pairs/{dex_chain}/{pair_address}"
    logger.info(f"==> DexScreener pairs request: {url}")
    
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        data = r.json()
        
        if "pair" in data and data["pair"] is not None:
            # Return as list for consistency
            return [data["pair"]]
        
        return []
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Error get_pair_by_address => {e}")
        return []

def parse_float(value, default=None):
    """
    Safely parse float values from various sources.
    Handles None, empty strings, and non-numeric strings.
    """
    if value is None:
        return default
    
    if isinstance(value, (float, int)):
        return float(value)
    
    try:
        # Remove common currency formatting
        if isinstance(value, str):
            value = value.replace(',', '')
            value = value.replace('$', '')
            value = value.strip()
            if value == '':
                return default
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Could not convert '{value}' to float")
        return default


```

### scripts/utils/common.py
```python
import logging

logger = logging.getLogger(__name__)

def parse_float(value, default=None):
    """
    Safely parse float values from various sources.
    Handles None, empty strings, and non-numeric strings.
    """
    if value is None:
        return default
    
    if isinstance(value, (float, int)):
        return float(value)
    
    try:
        # Remove common currency formatting
        if isinstance(value, str):
            value = value.replace(',', '')
            value = value.replace('$', '')
            value = value.strip()
            if value == '':
                return default
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Could not convert '{value}' to float")
        return default

```

### scripts/utils/__init__.py
```python

```

## sql Files

### sql/crypto_db_schema.sql
```sql
--
-- PostgreSQL database dump
--

-- Dumped from database version 14.16
-- Dumped by pg_dump version 14.16

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: timescaledb; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS timescaledb WITH SCHEMA public;


--
-- Name: EXTENSION timescaledb; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION timescaledb IS 'Enables scalable inserts and complex queries for time-series data (Community Edition)';


--
-- Name: telegram_groups_group_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.telegram_groups_group_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.telegram_groups_group_id_seq OWNER TO bot;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: telegram_groups; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.telegram_groups (
    group_id bigint DEFAULT nextval('public.telegram_groups_group_id_seq'::regclass) NOT NULL,
    telegram_id bigint NOT NULL,
    name text NOT NULL,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.telegram_groups OWNER TO bot;

--
-- Name: TABLE telegram_groups; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.telegram_groups IS 'Almacena los grupos de Telegram monitoreados por el bot.';


--
-- Name: telegram_messages_message_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.telegram_messages_message_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.telegram_messages_message_id_seq OWNER TO bot;

--
-- Name: telegram_messages; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.telegram_messages (
    message_id bigint DEFAULT nextval('public.telegram_messages_message_id_seq'::regclass) NOT NULL,
    group_id bigint NOT NULL,
    message_timestamp timestamp with time zone NOT NULL,
    raw_text text NOT NULL,
    sender_id bigint NOT NULL,
    is_call boolean DEFAULT false NOT NULL,
    reply_to_message_id bigint,
    token_id bigint,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.telegram_messages OWNER TO bot;

--
-- Name: TABLE telegram_messages; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.telegram_messages IS 'Registra todos los mensajes capturados de los grupos de Telegram.';


--
-- Name: token_calls_call_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.token_calls_call_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.token_calls_call_id_seq OWNER TO bot;

--
-- Name: token_calls; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.token_calls (
    call_id bigint DEFAULT nextval('public.token_calls_call_id_seq'::regclass) NOT NULL,
    token_id bigint NOT NULL,
    call_timestamp timestamp with time zone NOT NULL,
    call_price numeric(30,10) NOT NULL,
    message_id bigint,
    note text,
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.token_calls OWNER TO bot;

--
-- Name: TABLE token_calls; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.token_calls IS 'Almacena los calls espec√≠ficos de tokens detectados en mensajes.';


--
-- Name: tokens_token_id_seq; Type: SEQUENCE; Schema: public; Owner: bot
--

CREATE SEQUENCE public.tokens_token_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.tokens_token_id_seq OWNER TO bot;

--
-- Name: tokens; Type: TABLE; Schema: public; Owner: bot
--

CREATE TABLE public.tokens (
    token_id bigint DEFAULT nextval('public.tokens_token_id_seq'::regclass) NOT NULL,
    name text NOT NULL,
    ticker text NOT NULL,
    blockchain text NOT NULL,
    contract_address text NOT NULL,
    dex text,
    first_call_liquidity numeric(30,10),
    supply numeric(30,10) NOT NULL,
    initial_call_timestamp timestamp with time zone DEFAULT now(),
    group_call text,
    call_price numeric(30,10) NOT NULL,
    token_age integer,
    dexscreener_url text
);


ALTER TABLE public.tokens OWNER TO bot;

--
-- Name: TABLE tokens; Type: COMMENT; Schema: public; Owner: bot
--

COMMENT ON TABLE public.tokens IS 'Registra los tokens detectados en los mensajes de Telegram, con soporte para cualquier blockchain.';


--
-- Name: telegram_groups telegram_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_groups
    ADD CONSTRAINT telegram_groups_pkey PRIMARY KEY (group_id);


--
-- Name: telegram_groups telegram_groups_telegram_id_key; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_groups
    ADD CONSTRAINT telegram_groups_telegram_id_key UNIQUE (telegram_id);


--
-- Name: telegram_messages telegram_messages_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_messages
    ADD CONSTRAINT telegram_messages_pkey PRIMARY KEY (message_id);


--
-- Name: token_calls token_calls_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.token_calls
    ADD CONSTRAINT token_calls_pkey PRIMARY KEY (call_id);


--
-- Name: tokens tokens_contract_address_key; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.tokens
    ADD CONSTRAINT tokens_contract_address_key UNIQUE (contract_address);


--
-- Name: tokens tokens_pkey; Type: CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.tokens
    ADD CONSTRAINT tokens_pkey PRIMARY KEY (token_id);


--
-- Name: idx_telegram_groups_telegram_id; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_groups_telegram_id ON public.telegram_groups USING btree (telegram_id);


--
-- Name: idx_telegram_msg_group; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_msg_group ON public.telegram_messages USING btree (group_id);


--
-- Name: idx_telegram_msg_time; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_telegram_msg_time ON public.telegram_messages USING btree (message_timestamp DESC);


--
-- Name: idx_token_calls_time; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_token_calls_time ON public.token_calls USING btree (call_timestamp DESC);


--
-- Name: idx_token_calls_token; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_token_calls_token ON public.token_calls USING btree (token_id);


--
-- Name: idx_tokens_blockchain; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_tokens_blockchain ON public.tokens USING btree (blockchain);


--
-- Name: idx_tokens_contract; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_tokens_contract ON public.tokens USING btree (contract_address);


--
-- Name: idx_tokens_ticker; Type: INDEX; Schema: public; Owner: bot
--

CREATE INDEX idx_tokens_ticker ON public.tokens USING btree (ticker);


--
-- Name: telegram_messages telegram_messages_group_fk; Type: FK CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_messages
    ADD CONSTRAINT telegram_messages_group_fk FOREIGN KEY (group_id) REFERENCES public.telegram_groups(group_id);


--
-- Name: telegram_messages telegram_messages_token_fk; Type: FK CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.telegram_messages
    ADD CONSTRAINT telegram_messages_token_fk FOREIGN KEY (token_id) REFERENCES public.tokens(token_id);


--
-- Name: token_calls token_calls_msg_fk; Type: FK CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.token_calls
    ADD CONSTRAINT token_calls_msg_fk FOREIGN KEY (message_id) REFERENCES public.telegram_messages(message_id);


--
-- Name: token_calls token_calls_token_fk; Type: FK CONSTRAINT; Schema: public; Owner: bot
--

ALTER TABLE ONLY public.token_calls
    ADD CONSTRAINT token_calls_token_fk FOREIGN KEY (token_id) REFERENCES public.tokens(token_id);


--
-- PostgreSQL database dump complete
--


```